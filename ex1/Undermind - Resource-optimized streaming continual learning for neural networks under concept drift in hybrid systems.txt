REPORT CREATED ON
8/29/2025

Undermind

Research Report on

Resource-optimized streaming continual learning for neural networks under concept drift in hybrid systems
Full search query: I want to find journal articles from 2020–2025 on methodological advances in neural network incremental/continual learning for
streaming data that explicitly handle concept drift while optimizing compute time, memory footprint, energy usage, model size, and communication
cost in hybrid (distributed and centralized) settings, across any neural architecture

This is a Free Tier Undermind report. Limited results are shown. For complete analysis, upgrade to Pro.

Summary
No journal article from 2020–2025 squarely meets the full goal (online/streaming neural continual learning with explicit concept
drift handling while co-optimizing compute, memory, energy, model size, and communication in hybrid settings), but several works
partially cover key pieces—centralized drift-robust CL via feature/generative or buffer-free designs [3,6], and federated continual
learning with drift detection/adaptation but limited systems accounting [2,4,5].

What was found vs. what is still missing
• Found (partial coverage):
• Centralized/standalone online CL with explicit concept drift handling using:
• Variational domain-agnostic feature replay in a modular two-stream system (task and domain drift) [3].
• Buffer-free continual time-series learning via neural ODEs for irregular streams [6].
• Adaptive sliding windows with deep CNNs in streaming IoT [1].
• Streaming anomaly detection with teacher–student distillation to reduce update latency [10].
• Online predict-then-delayed-update neural learner grounded in time-delay regret [8].
• Multistream GNNs with real-time performance monitoring and subgraph adaptation [7].
• Federated/edge settings with continual adaptation and/or drift monitoring:
• Ensemble-and-Continual Federated Learning (heterogeneous client models, drift detection/adaptation) [2].
• Online FL with multiscale non-stationarity detection and dynamic-regret guarantees [5].
• Multi-head Federated Continual Learning at the edge (parameter isolation) [4].
• Missing (relative to the stated goal):
• No single paper provides a method that jointly handles concept drift and explicitly co-optimizes compute, memory, energy,
model size, and communication with rigorous, standardized reporting in a hybrid edge–cloud setup [1,2,3,4,5,6,7,8,9,10].
• Energy measurement and communication-byte accounting (with compression/sparsification) are notably absent; most
claims are qualitative or focus on accuracy/regret [1,2,3,4,5,6,7,8,9,10].

Closest fits by setting and their takeaways
• Centralized/standalone streaming CL:
• Two-stream modular CL with variational, domain-agnostic feature replay reduces raw-replay cost and addresses both covariate and label-conditional drift; strong conceptual fit for drift handling under bounded memory, but lacks compute/energy
metrics [3].
• ODEStream removes episodic buffers and handles irregular sampling via a temporal isolation layer and neural ODEs;
compelling for strict memory/latency regimes, but no systems metrics are reported [6].
• Sliding/adaptive windows bound memory and improve responsiveness in IoT streams; base model tuning is offline, and
system metrics are not quantified [1].
• Teacher–student distillation with student-only updates targets low-latency drift adaptation for anomaly detection; practical
lever for update cost, but no energy/compute/comm accounting [10].
• Time-delay regret (test-then-train with delayed updates) formalizes online update scheduling for high-velocity streams;
offers optimization-theoretic grounding but not systems co-design [8].
• Multistream GNN with dynamic graph generation and subgraph-level updates provides lightweight adaptation mechanisms for correlated streams; no resource accounting [7].
View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 1/30

Undermind

REPORT CREATED ON
8/29/2025

• Federated/hybrid (distributed) continual settings:
• Ensemble-and-Continual FL supports heterogeneous clients, drift detection, and adaptation; flexible but may inflate global
model size and does not optimize or report communication/energy [2].
• Online FL with non-stationarity detection combines FedAvg/FedOMD in a multiscale scheme with dynamic-regret guarantees; theoretically solid for drift but not communication- or energy-optimized in practice [5].
• Multi-head FCL at the edge offers parameter isolation to mitigate forgetting; helpful for plasticity/stability under heterogeneity, yet model size can grow and communication efficiency is unspecified [4].

Methodological advances relevant to resource constraints
• Memory/model-size control:
• Feature/generative replay to avoid raw sample storage [3].
• Buffer-free continual learning to eliminate replay memory [6].
• Sliding/adaptive windows to bound working memory [1].
• Parameter isolation via multi-heads (growth risk without consolidation) [4].
• Subgraph-level updates to limit adaptation scope in GNNs [7].
• Compute/latency levers:
• Student-only updates via knowledge distillation for faster adaptation [10].
• Predict-then-delayed-update scheduling grounded in time-delay regret [8].
• Multiscale detection to trigger adaptation in online FL (theory-backed) [5].
• Continuous-time ODE processing to handle irregular streams without costly re-sampling [6].
• Communication in hybrid settings:
• Federated ensemble aggregation for heterogeneous clients [2].
• Online FL with drift-aware adaptation across rounds [5].
• Multi-head FCL at the edge [4].
• Notably absent: explicit compression (quantized/sparse updates), adaptive upload triggers tied to drift, delta/adapter-only
broadcast, or MB/round reporting in these works [2,4,5].

Evaluation and metrics status
• Most papers report accuracy or regret; standardized online protocols (prequential test-then-train), recovery time after drift, and
stability–plasticity metrics appear conceptually but are inconsistently detailed [1,2,3,4,5,6,7,8,9,10].
• System metrics are largely missing:
• Compute/FLOPs per example and wall-clock latency: rarely quantified [1,2,3,4,5,6,7,8,9,10].
• Memory: qualitative or implicit via design (e.g., “buffer-free”); peak/working memory not standardized [1,3,6,7].
• Energy: none report joules per example or hardware-counter-based estimates [1,2,3,4,5,6,7,8,9,10].
• Communication: no MB/round or bits/step accounting; no compression/sparsification baselines [2,4,5].

Gaps and opportunities suggested by the corpus
• Drift-aware communication design: couple drift detectors with adaptive local steps, quantized/sparse gradient/weight updates,
and selective transmission of adapters/experts; report MB/round and energy/comm trade-offs [2,4,5].
• Unified resource accounting: adopt prequential online evaluation with compute (FLOPs/latency), memory (peak and episodic), energy (J/sample), and communication (MB/elapsed time) dashboards to make methods comparable across settings
[1,2,3,4,5,6,7,8,9,10].
• Consolidation under isolation/ensembles: integrate periodic distillation/pruning/merging to cap model size in multi-head or
ensemble FCL while preserving rapid drift adaptation [2,4].
• Minimal-memory CL with explicit budgets: extend buffer-free/feature-replay approaches with bounded adapter updates (e.g.,
LoRA/adapters) and optimizer-state compression; report memory vs. accuracy under drift [3,6].
• Cross-client/stream drift coordination: leverage multistream/graph ideas and analyses of client drift vs. forgetting to weight
aggregation by shared drift modes and to prioritize communication where it matters [7,9].

Paper-specific notes (most relevant points)
• Lao et al., TNNLS 2021: two-stream CL with variational domain-agnostic feature replay for both label-conditional and covariate
drift; strong drift handling with memory-aware replay, but no systems metrics [3].
•
View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 2/30

REPORT CREATED ON
8/29/2025

Undermind

Ganguly & Aggarwal, ToN 2022: online FL with non-stationarity detection and dynamic-regret bounds; nearest step toward
drift-aware hybrid learning with theory, lacking communication/energy optimization and reporting [5].
• Casado et al., Machine Learning 2023: ensemble-based FCL enabling heterogeneous clients and drift adaptation; practical
robustness, potential model-size and communication overhead unaddressed [2].
• ODEStream, TMLR 2024: buffer-free continual time-series learning via neural ODEs; promising for strict memory/latency but
missing standardized systems accounting [6].
• Chen et al., IJNC 2024: multi-head FCL at the edge; parameter isolation aids stability under continual updates but requires
size control and communication design [4].
• Others: adaptive sliding windows [1], KD-based low-latency anomaly detection [10], time-delay regret streaming updates [8],
multistream dynamic-graph adaptation [7], and analysis of client drift vs. forgetting interplay [9].

Categories
Scope Alignment and Relevance to the Goal
• Precisely relevant: none of the retrieved journal papers simultaneously deliver online continual neural learning with explicit
concept-drift handling and an explicit, measured co-optimization of compute, memory, energy, model size, and communication
in hybrid (distributed+centralized) settings.
• Somewhat related: several papers address online/streaming concept drift with neural methods, or federated/edge settings with
continual adaptation, but do not quantify all system-resource trade-offs or cover both centralized and distributed regimes.
• Distantly related: analysis-oriented or anomaly-detection works that touch drift and latency qualitatively without detailed
resource-control methodology; or methods that are continual/federated but underspecify drift or resource metrics.

Relevance categorization (objective fit vs. provided goal)
Paper

Online/single-pass CL

[1] Expert Systems 2023

[2] Machine
Learning 2023

Explicit drift
handling

Neural method

Hybrid (federated/edge)

Resource
optimization
levers

Reports
Category
resource
metrics
(compute/mem/energy/comm)

Partial (adaptive Yes (OASW)
sliding window;
base classifier
tuned offline)

Yes (CNN)

No

Claims
time/memory
efficiency via
OASW

No

Somewhat related

Yes (continual/federated)

Mixed (heterogeneous learners; includes
non-deep)

Yes (federated
ensemble)

Ensemble
aggregation
flexibility; not
comm/energy-focused

Likely not (not
specified)

Somewhat related

[3] IEEE TNNLS Yes (two-stream Yes (task and
2021
continual)
domain drift)

Yes (variational
replay, modular
NN)

No

Generative/feature replay;
modularization

No

Somewhat related

[4] Int. J. Netw.
Comput. 2024

Yes (federated
continual)

Implicit (continual adaptation;
not explicit drift
detectors)

Yes (multi-head
NN)

Yes
(edge–cloud)

Multi-head parameter isolation

No

Somewhat related

[5] IEEE/ACM
ToN 2022

Yes (online federated with detection)

Yes (non-stationarity detection + adaptation)

Model-agnostic; compatible
with NNs

Yes (federated)

Theoretical
framework; not
explicit
resource
co-design

No (dynamic
regret only)

Somewhat related

[6] TMLR 2024

Yes (buffer-free
continual)

Yes (concept
drift, irregular
sampling)

Yes (Neural
No
ODE + temporal
isolation)

Buffer-free
(memory-conscious)

No

Somewhat related

[7] IEEE Trans.
Cybern. 2025

Yes (online testing +
self-adaptation)

Yes (multi-stream drift;
graph updates)

Yes (GNN)

No

Lightweight
subgraph
updates
(claimed)

No

Somewhat related

[8] IEEE TPAMI
2025

Yes (dynamic stream; single-pass emphasis)

Implicit (addresses nonstationarity)

Yes (ConceptNN)

No

Online update
strategies
(one-by-one,
chunks)

No

Somewhat related

[9] Sci. Reports
2023

N/A (analysis
framework)

Yes (client drift + Uses deep
forgetting)
models in study

Yes (federated setting analyzed)

N/A (analysis,
not method)

N/A

Distantly related
(analysis)

Yes (drift detection + adaptation)

View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 3/30

REPORT CREATED ON
8/29/2025

Undermind

[10] IEEE IoT J.
2023

Yes (streaming
anomaly detection)

Yes (concept
drift focus)

Yes (KD-based)

Not specified
(appears centralized)

KD to reduce
update latency; dynamic parameter adjustments

Latency motivation; no
quantified system metrics

Somewhat related

Drift Types, Detectors, and Adaptation Mechanics
• Drift types addressed:
• Task/label-conditional and domain/covariate drift jointly: two-stream setting with variational domain-agnostic feature replay
[3].
• Nonstationarity in federated settings: client-side nonstationarity and server-side adaptation [2,5].
• Abrupt/ongoing drifts via sliding/adaptive windows or performance monitors: OASW [1], real-time error monitoring with
self-adaptation in GNNs [7], concept drift in streaming IoT anomaly detection [10], irregular temporal drifts handled by
Neural ODE dynamics [6].
• Combined spatial (client drift) and temporal drift analysis without new methods [9].
• Detection signals and gates:
• Adaptive/sliding window as drift surrogate [1].
• Non-stationarity detection integrated with multiscale online federated optimization [5].
• Online performance monitoring with trigger-driven subgraph updates [7].
• Distillation teacher–student with dynamic parameter adjustments to prioritize new distributions [10].
• Two-stream disentanglement with domain-agnostic representations reduces explicit detection reliance by routing via
modules [3].
• Adaptation mechanisms under drift:
• Replay: variational feature replay (domain-agnostic) [3].
• Memory/windowing: optimized adaptive sliding window [1]; buffer-free continual learning leveraging ODE latent dynamics
[6].
• Parameter isolation/heads: multi-head FCL with dedicated FC layers per task [4].
• Graph structural adaptation: subgraph updating and dynamic graph generator (AGG) with attention module for local
correlation changes [7].
• Online update regimes with time-delay regret and chunking vs one-by-one updates [8].
• Federated adaptation with aggregation that tolerates nonstationarity [2,5].

Online Protocols and Memory/Rehearsal Constraints
• Single-pass/prequential orientation:
• Emphasized for high-velocity streams and real-time responses [8]; online testing then updating is explicit in [7].
• Buffer-free design (no episodic buffers) to maintain responsiveness and bounded memory [6].
• Sliding/adaptive window processes a recent subset, approximating single-pass while bounding memory [1].
• Federated continual learning proceeds with ongoing client updates and aggregation [2,4,5].
• Replay/memory details:
• Feature/generative replay: variational domain-agnostic feature replay (reduces reliance on raw-input storage) [3].
• No-replay: ODEStream aims to avoid buffering altogether [6].
• Episodic memory specifics (size, policies) are generally not quantified in the papers above.

Resource-Optimization Levers and What Is Measured
• Levers proposed or implied (method-level):
• Memory:
• Buffer-free continual learning [6].
• Sliding window to bound working set [1].
• Multi-head parameter isolation (adds heads; could increase parameters per task) [4].
• Subgraph-level updates claimed lightweight vs full retrain [7].
• Feature-level replay (instead of raw data) [3].
• Compute/latency:
• Teacher–student distillation with student-only updates to reduce update latency [10].
View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 4/30

Undermind

REPORT CREATED ON
8/29/2025

• Online adaptation via multiscale OCO-inspired updates [5].
• ODE-based continuous-time modeling to process irregular sequences directly (avoids costly re-sampling) [6].
• Communication:
• Federated ensemble with heterogeneous local learners; flexible aggregation could impact communication payloads,
but no explicit compression or accounting is reported [2].
• Online federated optimization in [5] focuses on regret bounds; communication efficiency mechanisms are not
specified.
• Measurements reported (or not):
• None of the papers report comprehensive, standardized system metrics such as energy per example, FLOPs per example,
peak memory, or MB/round communication in prequential settings.
• [10] motivates reduced update latency qualitatively via KD; quantitative latency/energy numbers are not provided in the
abstract.
• [6] emphasizes buffer-free operation; memory reduction is methodological, not numerically benchmarked in the abstract.
• [2,4,5] in federated settings do not detail communication cost accounting in the provided summaries.

Centralized vs. Hybrid (Distributed) Settings and Communication Controls
• Centralized/standalone:
• Two-stream CL with variational replay [3], ODEStream [6], ConceptNN [8], CGLM [7], OASW [1], ADTCD [10] (appears
centralized).
• Hybrid/distributed (federated/edge):
• Ensemble and Continual Federated Learning using heterogeneous client models aggregated as an ensemble [2].
• Multi-head Federated Continual Learning for edge environments [4].
• Online Federated Learning with non-stationarity detection and adaptation, with dynamic regret guarantees [5].
• Communication mechanics:
• No explicit gradient/weight compression, sparsification, adaptive upload triggers, or MB/round accounting is described
in the summaries for [2,4,5]; [2] emphasizes flexibility in aggregating heterogeneous clients rather than bandwidth
minimization.
• Consequently, while [2,4,5] address federated continual settings with drift, they do not meet the “communication cost
optimization with explicit accounting” criterion.

Methodological Highlights (What’s Novel/Distinct)
• Variational domain-agnostic feature replay in a modular two-stream system that simultaneously addresses domain and task
drift [3].
• Federated ensemble continual learning that accommodates heterogeneous client model families/sizes with drift-aware adaptation, broadening applicability beyond deep-only setups [2].
• Online federated learning with non-stationarity detection tied to provable dynamic regret bounds combining FedAvg and
FedOMD regimes [5].
• Buffer-free continual learning for irregular time series via Neural ODEs with a temporal isolation layer, removing dependence
on long sequence buffers [6].
• Multi-head FCL providing parameter isolation for tasks in edge environments, mitigating forgetting while enabling continual
addition of heads [4].
• Graph-based self-adaptation for multistream drift via dynamic graph generation and adaptive diffusion attention, with subgraph-level update granularity [7].
• ConceptNN with time-delay regret, enabling real-time prediction then delayed update, and alternative online update granularities (one-by-one vs chunk-by-chunk) [8].
• Distillation-based adaptive anomaly detection with dynamic parameter adjustment to lower update latency and mitigate
self-poisoning in streaming IoT [10].
• Unified analysis framework for combined client drift and catastrophic forgetting to map performance landscapes (“generalization
bump”), guiding method design though not proposing a new algorithm [9].
• Optimized adaptive sliding window coupled with tuned deep CNN, focusing on addressing time/memory constraints via
windowing in streaming IoT analytics [1].

Comparative Tables
Drift handling and online protocol
View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 5/30

REPORT CREATED ON
8/29/2025

Undermind

Paper

Drift type focus

Detector/trigger

Adaptation mechanism

Replay/memory policy

Online protocol details

[1]

Concept drift in IoT
streams

Optimized adaptive
sliding window

Adjust window; tuned
DCNN classifier

Sliding window
(bounded)

Streaming with adaptive window; base
classifier tuned offline

[2]

Continual drift in federated clients

Drift detection and
adaptation mechanisms (not detailed)

Ensemble aggregation of heterogeneous
local learners

N/A (ensemble)

Continual federated
training

[3]

Task drift + domain
drift (two-stream)

Modular routing; not
explicit change-point
test

Variational doFeature/generative
main-agnostic feature replay
replay; modular solver

Continual two-stream
learning

[4]

Continual tasks at
edge

Not explicit

Multi-head parameter
isolation

Heads per task

Federated continual
learning

[5]

Non-stationarity in FL

Non-stationarity detection

Multiscale FedAvg/FedOMD with
adaptation

N/A

Online federated
with dynamic regret
bounds

[6]

Concept drift + irregular sampling

N/A (handled via dynamics)

ODE-based adaptor + Buffer-free (no replay
temporal isolation
buffers)

Continual, buffer-free
processing

[7]

Multistream drift (spatio-temporal)

Real-time performance monitoring

Dynamic graph generation; subgraph updates; ADGAT

Lightweight subgraph
changes

Online testing with
self-adaptation

[8]

Nonstationary
high-velocity streams

N/A (optimization-theoretic)

Time-delay regret;
online one-by-one or
chunk updates

N/A

Real-time predict, delayed update

[9]

Client drift + forgetting N/A
(analysis)

N/A

N/A

Analysis framework
only

[10]

Concept drift in IoT
anomaly detection

No explicit buffer;
self-poisoning control

Streaming anomaly
detection with reduced
update latency

Dynamic paraTeacher–student KD
meter adjustment;
with student updates
OCSVM-based outlier only
removal

Resource levers and reported system metrics
Paper

Compute/latency
lever

Memory/model-size
lever

Energy

Communication
lever

Reported quantitative system metrics

[1]

Adaptive window to
limit processing

Sliding window
bounds working set

Not reported

N/A

Accuracy metrics only
(time/memory claims
qualitative)

[2]

Ensemble aggregation; not latency-optimized

Heterogeneous models in ensemble

Not reported

Federated training; no
compression/MB accounting

Not reported

[3]

Modular computation;
replay cost

Feature/generative
replay vs raw storage

Not reported

N/A

Not reported

[4]

N/A

Multi-head growth
(parameter isolation)

Not reported

Federated; no comm
optimization stated

Not reported

[5]

Online OCO-based
updates

N/A

Not reported

Standard FL; no compression/adaptive upload stated

Dynamic regret
bounds only

[6]

Direct processing of
irregular data via
ODEs

Buffer-free (no
episodic memory)

Not reported

N/A

Not reported

[7]

Subgraph updates
(local changes)

Lightweight subgraphs vs full graph

Not reported

N/A

Not reported

[8]

Delayed update
scheduling

N/A

Not reported

N/A

Not reported

[9]

N/A

N/A

N/A

N/A

N/A

[10]

KD with student-only
updates to lower latency

N/A

Not reported

N/A

Not reported

Notes:

View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 6/30

REPORT CREATED ON
8/29/2025

Undermind

• None of the papers quantify energy consumption; none provide communication-bytes accounting or gradient compression in
the abstracts provided [1,2,3,4,5,6,7,8,9,10].
• Memory-related mechanisms are present in [1,3,6,7], but without standardized peak/working memory measurements [1,3,6,7].
• Federated papers [2,4,5] lack explicit communication-optimization techniques or metrics in the provided summaries.

Hybrid/distributed design specifics
Paper

Federated/hybrid
design

Client heterogeneity

Asynchrony/staleness

Communication
Server-side drift
compression/sparsi- logic
fication

[2]

Yes (ensemble of
client models)

Yes (heterogeneous
learners, sizes, even
non-deep)

Not specified

Not specified

Yes (drift detection +
adaptation at system
level)

[4]

Yes (FCL at edge)

Not specified

Not specified

Not specified

Not specified

[5]

Yes (online FL with
detection)

Not specified

Not specified

Not specified

Yes (non-stationarity
detection with multiscale adaptation)

Takeaways for Experts
• Methodological coverage of concept drift in online neural continual learning is present across centralized [1,3,6,7,8,10] and
federated [2,4,5] settings, but explicit, measured co-optimization of compute, memory, energy, model size, and communication
is largely absent in these journal articles’ abstracts [1,2,3,4,5,6,7,8,9,10].
• If hybrid resource-aware co-design is the priority, [2,5] are the closest federated continual/drift-aware directions to build upon;
however, communication and system metrics would need to be added in future work.
• For minimal-memory continual adaptation, ODEStream’s buffer-free design [6] and feature-replay in a modular two-stream
architecture [3] offer complementary strategies; neither provides energy/latency/compute accounting in the summaries.
• Parameter isolation via multi-heads in FCL [4] and subgraph-local updates in GNNs [7] suggest structural sparsity avenues for
compute and communication reduction, but explicit budgets/measurements are not provided.
• Analysis in [9] highlights interactions between client drift and forgetting that may inform drift-aware aggregation policies and
replay scheduling in hybrid systems, though it is not a method.
Overall, these works collectively advance drift-aware continual learning across several architectures and settings, but there is a clear
gap in rigorous, multi-metric resource evaluation and explicit communication-efficient design in hybrid edge–cloud scenarios.

Timeline
Timeline of key developments (2021–2025)
• 2021: Modular two-stream continual learning with feature-space generative replay
• A two-stream CL system explicitly separates task drift (P(y|x)) and domain drift (P(x)) and introduces variational
domain-agnostic feature replay to stabilize learning across both streams. This marks an early journal milestone in
using modularization (inference/generative/solver) and feature-level replay to handle heterogeneous drift while containing
rehearsal costs [3].
• 2022: Online federated learning with principled non-stationarity detection and dynamic regret guarantees
• A multi-scale FL framework integrates drift detection with adaptation atop FedAvg/FedOMD, providing dynamic regret
bounds under concept drift. This is a notable step toward theoretically grounded drift-aware FL and begins to couple
online adaptation with communication-aware aggregation in distributed settings [5].
• 2023: Broadening to ensemble-based FCL and empirical analysis of joint client and concept drift
• Ensemble and Continual Federated Learning proposes an ensemble-as-global-model to flexibly aggregate heterogeneous client learners and includes drift detection/adaptation for non-stationary federated streams—highlighting practical
heterogeneity and robustness concerns but at the cost of larger model size [2].
• A controlled framework to jointly vary spatial (client) and temporal (concept) shift shows interactions between client drift
and forgetting (“Generalization Bump”), underscoring that drift axes must be studied together in hybrid learning scenarios
[9].
• Domain-specific online drift adaptation emerges in IoT anomaly detection (ADTCD) via teacher–student distillation and
dynamic parameter weighting to reduce update latency and mitigate self-poisoning—illustrating low-latency drift response
under streaming constraints [10].
• Sliding-window deep learning for concept drift with adaptive window sizing appears in centralized settings, emphasizing
time/memory constraints alongside drift-aware adaptation, albeit with limited hybrid resource accounting [1].
• 2024: Buffer-free continual time-series learning and multi-head FCL for edge
View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 7/30

Undermind

REPORT CREATED ON
8/29/2025

• ODEStream introduces a buffer-free continual framework for irregular streaming time series via neural ODEs and a
temporal isolation layer, explicitly targeting memory/latency and irregular sampling while learning evolving dynamics—an
inflection point for strict-memory settings without replay buffers [6].
• A multi-head federated continual learning architecture for edge computing emphasizes privacy and robustness with
task-specific heads in a federated pipeline, reflecting a move toward parameter isolation in hybrid systems, though with
potential model-size growth [4].
• 2025: Toward multi-stream graph adaptation and online regret-driven neural updates
• Continuous Graph Learning for multistream concept drift adapts correlation graphs online via a dynamic generator and
diffusion-attention, addressing inter-stream dependencies and real-time drift monitoring—expanding drift handling beyond
single-stream assumptions [7].
• ConceptNN introduces a concept-space representation with time-delay regret (test-then-train) and online update regimes
(one-by-one, chunk-by-chunk), contributing optimization-theoretic underpinnings for single-pass neural stream learners
[8].

Methodological themes and shifts
• From replay of raw data to feature/generative replay and buffer-free designs
• Early neural CL under drift embraced feature-level generative replay to limit memory and stabilize representation sharing
across task/domain drift [3].
• A counter-trend focuses on buffer-free continual learners to minimize memory and latency, leveraging continuous-time
dynamics (neural ODEs) and temporal isolation instead of storage-heavy replay [6].
• Explicit drift detection gating adaptation in centralized and federated settings
• Multi-scale detection intertwined with federated aggregation arrives with dynamic regret guarantees, operationalizing
adaptation schedules under drift in distributed contexts [5].
• Practical drift detectors/monitors are integrated with ensembles in FCL [2], with online performance monitors triggering
self-adaptation in multistream GNNs [7], and teacher–student refresh policies in IoT anomaly detection to reduce update
latency [10].
• Parameter isolation and modularity for stability–plasticity and heterogeneity
• Modular architectures (separate inference/generative/solver) handle mixed drift types [3].
• Multi-head isolation in FCL addresses privacy, catastrophic forgetting, and heterogeneous tasks at the edge, aligning with
conditional computation trends though requiring consolidation to curb growth [4].
• Accounting for hybrid (edge–cloud) realities and non-IID, asynchronous dynamics
• FL methods move from stationary assumptions to explicit concept drift handling with principled regret and adaptation [5].
• Ensemble FCL emphasizes flexible aggregation of heterogeneous client models and drift adaptation in practice [2], and
empirical analyses connect client drift with temporal drift impacts [9].
• From single-stream to interdependent multi-stream drift
• Multistream frameworks with dynamic graph construction and subgraph adaptation reflect the move toward correlated,
real-world sensor/edge streams where inter-stream relations also drift [7].

Resource-efficiency and systems considerations across the timeline
• Memory and model size
• Feature-space generative replay reduces raw data storage versus exemplar buffers [3].
• Buffer-free ODE-based continual learners eliminate replay memory entirely, trading for continuous-time computation and
potentially lower memory footprint during inference/training [6].
• Multi-head FCL increases parameter counts but reduces per-task interference; suggests the need for pruning/merging or
distillation-based consolidation to contain model size [4].
• Sliding-window adaptive learners balance window length against memory and responsiveness [1].
• Compute/latency and online updates
• Teacher–student designs update only the student for quick response and reduced per-update cost [10].
• Multi-scale detection with dynamic regret prioritizes timely adaptation and aligns local-update windows with communication constraints in FL [5].
• ODEStream targets real-time processing of irregular sequences without buffering, emphasizing low-latency single-pass
operation [6].
• Communication in hybrid/distributed settings
• Drift-aware FL with theoretical grounding addresses communication-efficiency implicitly through local steps and adaptive
aggregation, though explicit compression/MB accounting is limited in these works [5].
View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 8/30

Undermind

REPORT CREATED ON
8/29/2025

• Ensemble FCL and multi-head FCL highlight flexible aggregation and privacy at the edge but provide fewer concrete
communication-cost controls; they nevertheless reflect a shift from centralized-only streaming toward hybrid deployment
[2,4].
• Energy usage
• Direct energy measurement is largely absent across these journal papers; energy is only indirectly addressed via
memory/computation reductions (e.g., buffer-free learning, limited updates, modular/gated updates) [3,5,6,10]. This
indicates a persistent gap relative to the stated efficiency goals.
Overall, resource-conscious design evolves from storage-heavy replay toward feature/generative replay and then to buffer-free
continuous-time modeling; federated settings progressively integrate drift detection with principled adaptation, while parameter
isolation (multi-head) and modularity improve stability under heterogeneity [2,3,4,5,6].

Trends and their significance
• Integration of drift handling into federated/edge learning is accelerating
• The field moves from centralized CL with drift modeling to federated frameworks that explicitly detect and adapt to drift,
with initial theoretical guarantees for non-stationary environments [5]. Ensemble and multi-head FCL extend practical
robustness to heterogeneous devices [2,4].
• Emphasis on reduced memory and immediate responsiveness
• Buffer-free sequential models for irregular streams [6] and teacher–student latency-aware updates [10] reflect an
operational turn toward strict online constraints and real-time deployment.
• From drift-in-x or drift-in-y to mixed and multistream drift
• Two-stream decoupling of domain and task drift [3] and multistream graph adaptation [7] signal a maturation toward richer,
real-world drift taxonomies and dependencies.
• Methodological pluralism under bounded resources
• Coexisting strategies—feature/generative replay, parameter isolation, modular two-stream designs, online regret-driven
updates—suggest no single dominant paradigm, but rather a toolkit adapted to domain constraints and hybrid systems
realities [3,4,5,6,8].

Contributing groups and recurring threads
• Variational/feature replay for robust CL under mixed drift
• Lao et al. (with Bengio) introduced variational domain-agnostic feature replay in a modular two-stream system, influencing
later emphasis on representation-level stability under heterogeneous drift [3].
• Drift-aware federated learning with theoretical grounding
• Ganguly and Aggarwal’s multi-scale detection plus dynamic-regret analysis marks a rigorous step for online FL under drift
and sets a template for coupling adaptation schedules with aggregation in non-stationary regimes [5].
• Practical FCL for heterogeneous edge environments
• Casado et al. develop ensemble-based FCL for continual classification with drift adaptation and heterogeneous clients
[2].
• Chen et al. advocate multi-head FCL for privacy and robustness at the edge, reinforcing parameter isolation trends in
hybrid settings [4].
• Streaming time-series and multistream modeling
• Abushaqra and Salim’s ODEStream advances buffer-free continual forecasting for irregular streams [6].
• Zhou and Lu’s continuous graph learning targets inter-stream drift via dynamic graphs and attention diffusion [7].
• Optimization-theoretic stream learners
• Mi’s ConceptNN with time-delay regret and online update regimes (one-by-one, chunk) anchors streaming neural updates
in online optimization theory, aligning with prequential test-then-train practices [8].
These clusters indicate a convergence between theoretically grounded drift-adaptive FL, representation-centric stabilization, and
domain-specific streaming architectures (time-series, graphs). Future consolidation is likely around hybrid systems that jointly
optimize memory, compute, and communication under explicit drift models.

Gaps and likely next milestones suggested by the timeline
• Unified resource accounting in hybrid CL under drift
• Few works report comprehensive metrics (compute/latency, peak memory, energy per sample, and MB/round) in
prequential protocols; expect upcoming studies to standardize system-level reporting and co-design local adaptation with
communication policies [2,4,5,6,10].
• Energy-first continual learning
View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 9/30

REPORT CREATED ON
8/29/2025

Undermind

• Direct energy measurements and energy-aware schedulers (e.g., adaptive precision, sparse experts, early exiting gated
by drift) are largely missing and represent a clear opportunity.
• Consolidation and model-size control for isolation/ensemble approaches
• Multi-head and ensemble FCL need budget-aware consolidation (distillation, pruning/merging) to prevent growth; integrating such cycles into drift-aware triggers would advance deployability [2,4].
• Drift-aware compression and selective communication
• Bridging drift detectors with adaptive upload, compression, and expert/adapters-only transmissions should reduce
uplink/downlink costs while maintaining global adaptation in hybrid settings—an area not explicitly quantified in these
papers [2,4,5].
• Cross-stream and cross-client drift coordination
• Building on multistream graph adaptation [7] and joint client–concept drift analyses [9], future systems may weight or route
updates according to shared drift modes across clients/streams to improve global stability and communication efficiency.
In sum, 2021–2025 shows clear progression from centralized drift-robust CL via representation replay [3], to drift-aware federated
learning with theoretical guarantees [5], to practical FCL under heterogeneity [2,4], alongside advances in buffer-free time-series
streaming [6] and multistream graph adaptation [7]. The next phase will likely integrate rigorous resource accounting and drift-aware
communication with consolidation mechanisms to meet real hybrid deployment budgets.

Adjacent Work
Which papers cite the same foundational papers as relevant papers?
Use this table to discover related papers on adjacent topics, to gain a broader understanding of the field and help generate ideas
for useful new research directions.
Ref.

Adjacency
score

Title

References These Foundational Papers

[90]

0.00

The Importance of Being Lazy: Scaling Limits of Continual Learning

[133]

[48]

0.00

Improving Performance in Continual Learning Tasks using Bio-Inspired
Architectures

[65, 83, 105, 122, 133]

[12]

0.00

Drift Detection and Adaptation for Federated Learning in IoT with Adap- [5, 78, 112, 133, 136]
tive Device Management

[139]

0.00

Continual Learning Beyond Experience Rehearsal and Full Model Sur- [60, 117, 122, 127, 133]
rogates

[120]

0.00

Statistically Valid Post-Deployment Monitoring Should Be Standard for
AI-Based Digital Health

[133]

[126]

0.00

Optimizers Qualitatively Alter Solutions And We Should Leverage This

[83, 127, 133]

[36]

0.00

Holistic Continual Learning under Concept Drift with Adaptive Memory
Realignment

[52, 60, 61, 96, 102, 117, 131, 136, 140]

[56]

0.00

Mitigating Forgetting in Online Continual Learning via Instance-Aware
Parameterization

[60, 65, 82, 92, 96, 121, 122]

[47]

0.00

Multilayer Neuromodulated Architectures for Memory-Constrained Online Continual Learning

[92, 102, 117, 133]

[61]

0.00

Is Class-Incremental Enough for Continual Learning?

[60, 117, 121, 127, 133]

[78]

0.00

Non-IID data and Continual Learning processes in Federated Learning: [102, 117, 133]
A long road ahead

[54]

0.00

A Simple Baseline for Stable and Plastic Neural Networks

[102, 133]

[31]

0.00

Class-Incremental Experience Replay for Continual Learning under
Concept Drift

[57, 77, 110, 121, 133]

[138]

0.00

Exploring Kolmogorov-Arnold Network Expansions in Vision Transform- [61, 122, 133]
ers for Mitigating Catastrophic Forgetting in Continual Learning

[89]

0.00

Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting

[181]

0.00

Contrastive Regularization over LoRA for Multimodal Biomedical Image [122, 133]
Incremental Learning

[25]

0.00

Federated Continual Learning via Knowledge Fusion: A Survey

[28, 78, 83, 85, 112, 127]

[122]

0.00

Expert Gate: Lifelong Learning with a Network of Experts

[117]

[102, 117, 127, 133]

View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 10/30

REPORT CREATED ON
8/29/2025

Undermind

[184]

0.00

A Survey of Continual Reinforcement Learning

[25, 102, 133]

[188]

0.00

GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM
via General Samples Replay

[117, 133]

View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 11/30

REPORT CREATED ON
8/29/2025

Undermind

References
[1] Concept drift detection and adaption framework using optimized deep learning and adaptive sliding window approach
K. Desale and S. Shinde. Expert Systems, 2023. 5 citations.
32% Topic Match
Proposes a concept-drift-aware streaming framework using optimized deep CNN and adaptive sliding windows.
Uses a hybrid “aggressive hunt optimization” to tune a Deep CNN (offline-trained) plus an optimized adaptive sliding window (OASW) to detect/adapt drift while
claiming time/memory efficiency.
Lacks explicit online single-pass continual learning protocol, hybrid (edge–cloud) or communication/energy accounting; evaluations report classification metrics (no
FLOPs, memory/energy, online accuracy, rehearsal, or distributed experiments).

[2] Ensemble and continual federated learning for classification tasks
F. Casado, ..., and S. Barro. Machine Learning, 2023. 15 citations.
28% Topic Match
Proposes an ensemble-based federated continual learning architecture for streaming classification with drift.
Implements heterogeneous client models aggregated into a global ensemble, combined with drift detection and local adaptation.
Journal article (2023) but focuses on ensemble/heterogeneous models in federated continual classification; reports drift handling and continual adaptation but unclear
if it uses strict single-pass/online prequential protocol or measures compute, memory, energy, model size, and communication accounting for hybrid (edge–cloud)
settings.

[3] A Two-Stream Continual Learning System With Variational Domain-Agnostic Feature Replay
Qicheng Lao, ..., and Y. Bengio. IEEE Transactions on Neural Networks and Learning Systems, 2021. 22 citations.
21% Topic Match
Proposes a two-stream continual learning system with variational domain-agnostic feature replay.
Implements an inference module (domain-agnostic representations), a generative replay module (variational feature generator), and a solver module to handle task
(conditional) and domain (marginal) drift across support/query streams.
Journal (2021, IEEE TNNLS); focuses on modular drift handling and feature-level replay but does not explicitly report single-pass streaming constraints,
online/prequential protocol, resource metrics (compute/time, energy, memory budgets, model size), or hybrid distributed communication considerations—likely
centralized and not optimized for constrained/edge/hybrid settings.

[4] A Multi-Head Federated Continual Learning Approach for Improved Flexibility and Robustness in Edge Environments
Chunlu Chen, ..., and Kouichi Sakurai. Int. J. Netw. Comput., 2024. 1 citations.
16% Topic Match
Proposes a multi-head federated continual learning (FCL) framework for edge environments.
Combines federated learning with task-specific fully connected heads to preserve past-task features and enable continual updates at clients.
Likely targets privacy and robustness (including adversarial training); paper is 2024 journal but unclear if it uses single-pass/streaming protocol, explicit concept-drift
detection, online/prequential evaluation, resource/communication accounting (MB/round), or hybrid edge–cloud compression strategies—so relevance to strict
streaming/drift+resource optimization criteria is uncertain.

[5] Online Federated Learning via Non-Stationary Detection and Adaptation Amidst Concept Drift
Bhargav Ganguly and V. Aggarwal. IEEE/ACM Transactions on Networking, 2022. 12 citations.
15% Topic Match
Proposes a federated online-learning framework with drift detection and adaptation.
Combines FedAvg/FedOMD with multiscale nonstationarity detection to trigger local/server adaptation, proving dynamic-regret bounds under concept drift.
Journal (2022) targets distributed/hybrid FL with explicit drift handling and regret analysis, but does not focus on neuralspecific mechanisms, singlepass replay, or
measured compute/memory/energy/modelsize/communication accounting.

[6] ODEStream: A Buffer-Free Online Learning Framework with ODE-based Adaptor for Streaming Time Series Forecasting
Futoon M. Abushaqra, ..., and Flora D. Salim. Trans. Mach. Learn. Res., 2024. 0 citations.
13% Topic Match
Proposes a buffer-free online continual-forecasting framework using ODE-based adaptors.
Implements a temporal-isolation layer plus Neural ODEs to process irregular streaming time series and adapt to concept drift without replay buffers.
Relevant for streaming drift and bounded-memory (no episodic buffer) but does not report compute/energy/model-size/communication metrics or hybrid (edge–cloud)
deployment details.

[7] Continuous Graph Learning-Based Self-Adaptation for Multi-Stream Concept Drift
Ming Zhou and Jie Lu. IEEE Transactions on Cybernetics, 2025. 1 citations.
10% Topic Match
Proposes a continuous graph-learning self-adaptation framework (CGLM) for multistream concept drift.
Uses a GNN with a dynamic graph generator (AGG) and ADGAT to detect drift online and update/replace subgraphs using small-scale historical and incoming
samples.
Journal article (2025) targets multistream concept drift with lightweight subgraph updates, but does not mention singlepass/online resource accounting (compute,
memory, energy), hybrid edge–cloud communication, boundedmemory replay, or explicit streaming/continual learning constraints—check full text for online protocol,
latency, and resource/communication measurements.

[8] Concept Neural Network Based on Time-Delay Regret for Dynamic Stream Learning
Yunlong Mi. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. 2 citations.
8% Topic Match
Proposes a Concept Neural Network (ConceptNN) with time-delay regret for dynamic stream learning.
Builds a concept space (feature intent + weight extent) and uses real-time-predict then delayed-update online optimization with one-by-one and chunk updates.
Journal (2025, IEEE TPAMI) targets single-pass streaming and online updates, but paper does not mention explicit concept-drift detectors, resource metrics
(compute/energy/memory/model size/communication), or hybrid distributed settings — likely relevant for streaming protocol but not for resource- or drift-aware
hybrid optimization.

[9] Jointly exploring client drift and catastrophic forgetting in dynamic learning
Niklas Babendererde, ..., and A. Mukhopadhyay. Scientific Reports, 2023. 4 citations.
6% Topic Match
Proposes a unified analysis framework linking client drift and catastrophic forgetting.
Builds a controlled 3D testbed modeling spatial (client) and temporal (stream) distribution shifts, producing performance landscapes and spotting a “generalization
bump.”
Relevant? It is a 2023 journal article on federated+continual dynamics, but it is analytical/empirical rather than proposing streaming neural-methods that explicitly
optimize compute/memory/energy/model size or hybrid communication protocols.

[10] ADTCD: An Adaptive Anomaly Detection Approach Toward Concept Drift in IoT
Lijuan Xu, ..., and Xin Li. IEEE Internet of Things Journal, 2023. 22 citations.
6% Topic Match

View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 12/30

Undermind

REPORT CREATED ON
8/29/2025

Proposes an adaptive, knowledge-distillation anomaly detector for IoT streams under concept drift.
Uses a teacher–student setup: only the student is updated via dynamic weight adjustment on new samples; OCSVM removes outliers to avoid self-poisoning.
Relevant? Partially: targets online streaming drift and update latency, but focuses anomaly detection (one-class), does not report neural-network resource metrics
(compute/energy/communication/model-size) nor hybrid edge–cloud communication accounting.

[11] A Multi-Model Approach for Handling Concept Drifting Data in Federated Learning
Guanhui Yang, ..., and Shuo Wang. 2024 20th International Conference on Mobility, Sensing and Networking (MSN), 2024. 2 citations.
4% Topic Match
Abstract: Federated learning (FL) is a distributed machine learning framework that trains a global model with local model updates from multiple client devices. When
learning from data streams, the performance of FL models can be significantly degraded, because of concept drift and its subsequent issue of data heterogeneity.
Concept drift refers to changes in data distribution. Concept drift in FL can be temporal and spatial, leading to outdated models under new data distributions and
poorer model performance from exacerbated heterogeneity. We propose a multi-model approach (FedMCD) that keeps two local models at each client to balance
the global and local performance...

[12] Drift Detection and Adaptation for Federated Learning in IoT with Adaptive Device Management
Shuang Zhou, ..., and A. Gokhale. 2024 IEEE International Conference on Big Data (BigData), 2024. 1 citations.
4% Topic Match
Abstract: Federated learning (FL) is a promising approach for edge/IoT-based distributed machine learning, where both privacy and bandwidth efficiency are
essential. However, as time progresses, edge/IoT-based FL faces challenges such as unpredictable concept drift, leading to model performance degradation and
the need for frequent retraining. To address these challenges, we propose a federated learning framework designed for heterogeneous IoT devices, capable of
handling continuous data distribution changes while accounting for limited storage resources. Our framework introduces a server-side drift detection method to
minimize bandwidth usage and optimize retraining times, conserving IoT device resources. We also present an efficient storage management strategy...

[13] FedGAMMA: Federated Learning With Global Sharpness-Aware Minimization
Rong Dai, ..., and Yongdong Zhang. IEEE Transactions on Neural Networks and Learning Systems, 2023. 30 citations.
3% Topic Match
Abstract: Federated learning (FL) is a promising framework for privacy-preserving and distributed training with decentralized clients. However, there exists a large
divergence between the collected local updates and the expected global update, which is known as the client drift and mainly caused by heterogeneous data
distribution among clients, multiple local training steps, and partial client participation training. Most existing works tackle this challenge based on the empirical risk
minimization (ERM) rule, while less attention has been paid to the relationship between the global loss landscape and the generalization ability. In this work, we
propose FedGAMMA, a novel FL algorithm with Global...

[14] Flash: Concept Drift Adaptation in Federated Learning
Kunjal Panchal, ..., and Hui Guan. Unknown journal, 2023. 22 citations.
2% Topic Match
No summary or abstract available

[15] Online Training from Streaming Data with Concept Drift on FPGAs
Esther Roorda and S. Wilton. 2023 24th International Symposium on Quality Electronic Design (ISQED), 2023. 1 citations.
2% Topic Match
Abstract: In dynamic environments, the inputs to machine learning models may exhibit statistical changes over time, through what is called concept drift. Incremental
training can allow machine learning models to adapt to changing conditions and maintain high accuracy by continuously updating network parameters. In the context
of FPGA-based accelerators however, online incremental learning is challenging due to resource and communication constraints, as well as the absence of labelled
training data. These challenges have not been fully evaluated or addressed in existing research. In this paper, we present and evaluate strategies for performing
incremental training on streaming data with concept drift on...

[16] Distributed Networked Real-Time Learning
Alfredo García, ..., and Lingzhou Hong. IEEE Transactions on Control of Network Systems, 2020. 4 citations.
2% Topic Match
Abstract: Many machine learning algorithms have been developed under the assumption that datasets are already available in batch form. Yet, in many application
domains, data are only available sequentially overtime via compute nodes in different geographic locations. In this article, we consider the problem of learning a
model when streaming data cannot be transferred to a single location in a timely fashion. In such cases, a distributed architecture for learning which relies on a
network of interconnected “local” nodes is required. We propose a distributed scheme in which every local node implements stochastic gradient updates based
upon a local data stream....

[17] Exploring System Performance of Continual Learning for Mobile and Embedded Sensing Applications
Young D. Kwon, ..., and C. Mascolo. 2021 IEEE/ACM Symposium on Edge Computing (SEC), 2021. 34 citations.
1% Topic Match
Abstract: Continual learning approaches help deep neural network models adapt and learn incrementally by trying to solve catastrophic forgetting. However,
whether these existing approaches, applied traditionally to image-based tasks, work with the same efficacy to the sequential time series data generated by mobile
or embedded sensing systems remains an unanswered question. To address this void, we conduct the first comprehensive empirical study that quantifies the
performance of three predominant continual learning schemes (i.e., regularization, replay, and replay with examples) on six datasets from three mobile and embedded
sensing applications in a range of scenarios having different learning complexities. More specifically, we...

[18] Asynchronous Federated Learning for Sensor Data with Concept Drift
Yujing Chen, ..., and H. Rangwala. 2021 IEEE International Conference on Big Data (Big Data), 2021. 37 citations.
1% Topic Match
Abstract: Federated learning (FL) involves multiple distributed devices jointly training a shared model without any of the participants having to reveal their local data to
a centralized server. Most of previous FL approaches assume that data on devices are fixed and stationary during the training process. However, this assumption is
unrealistic because these devices usually have varying sampling rates and different system configurations. In addition, the underlying distribution of the device data
can change dynamically over time, which is known as concept drift. Concept drift makes the learning process complicated because of the inconsistency between
existing and upcoming data. Traditional concept...

[19] Resource-Efficient Heterogenous Federated Continual Learning on Edge
Zhao Yang, ..., and Meng Zhang. 2024 Design, Automation & Test in Europe Conference & Exhibition (DATE), 2024. 0 citations.
1% Topic Match
Abstract: Federated learning (FL) has been widely deployed on edge devices. In practical, the data collected by edge devices exhibits temporal variations. This leads
to catastrophic forgetting issue. Continual learning methods can be used to address this problem. However, when deploying these methods in FL on edge devices,
it is challenging to adapt to the limited resources and heterogeneous data of the deployed devices, which reduces the efficiency and effectiveness of federated
continual learning (FCL). Therefore, this article proposes a resource-efficient heterogeneous FCL framework. This framework divides the global model into an
adaptation part for new knowledge and a preservation part...

[20] Federated Learning under Distributed Concept Drift
Ellango Jothimurugesan, ..., and Phillip B. Gibbons. ArXiv, 2022. 58 citations.
1% Topic Match
Abstract: Federated Learning (FL) under distributed concept drift is a largely unexplored area. Although concept drift is itself a well-studied phenomenon, it poses
particular challenges for FL, because drifts arise staggered in time and space (across clients). To the best of our knowledge, this work is the first to explicitly
study data heterogeneity in both dimensions. We first demonstrate that prior solutions to drift adaptation that use a single global model are ill-suited to staggered
drifts, necessitating multiple-model solutions. We identify the problem of drift adaptation as a time-varying clustering problem, and we propose two new clustering
algorithms for reacting to...

[21] Adaptive and Reinforcement Learning Approaches for Online Network Monitoring and Analysis
View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 13/30

Undermind

REPORT CREATED ON
8/29/2025

Sarah Wassermann, ..., and P. Casas. IEEE Transactions on Network and Service Management, 2021. 9 citations.
1% Topic Match
Abstract: Network-monitoring data commonly arrives in the form of fast and changing data streams. Continuous and dynamic learning is an effective learning strategy
when dealing with such data, where concept drifts constantly occur. We propose different stream-based, adaptive learning approaches to analyze network-traffic
streams on the fly. We address two major challenges associated to stream-based machine learning and online network monitoring: (i) how to dynamically learn
from and adapt to non-stationary data changing over time, and (ii) how to deal with the limited availability of labeled data to continuously tune a supervised-learning
model. We introduce ADAM & RAL, two stream-based machine-learning...

[22] Latent Replay for Real-Time Continual Learning
Lorenzo Pellegrini, ..., and D. Maltoni. 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2019. 168 citations.
1% Topic Match
Abstract: Training deep neural networks at the edge on light computational devices, embedded systems and robotic platforms is nowadays very challenging. Continual
learning techniques, where complex models are incrementally trained on small batches of new data, can make the learning problem tractable even for CPU-only
embedded devices enabling remarkable levels of adaptiveness and autonomy. However, a number of practical problems need to be solved: catastrophic forgetting
before anything else. In this paper we introduce an original technique named "Latent Replay" where, instead of storing a portion of past data in the input space, we
store activations volumes at some intermediate layer....

[23] Adaptive Federated Learning in Presence of Concept Drift
Giuseppe Canonaco, ..., and M. Roveri. 2021 International Joint Conference on Neural Networks (IJCNN), 2021. 34 citations.
1% Topic Match
Abstract: Federated Learning (FL) is a promising research area in the machine learning field. Techniques and solutions belonging to this area operate in distributed
scenarios, comprising a server and pervasively distributed clients, aiming at learning a single central model without sending (possibly sensitive) data from the clients
to the server. Such an approach allows mitigating the privacy concerns that are nowadays perceived as relevant in distributed machine learning solutions leveraging
data belonging to different users or companies. The literature in the field of FL is wide and many state-of-the-art solutions are available. Unfortunately, all these
solutions assume (implicitly or explicitly) that...

[24] Experience Replay as an Effective Strategy for Optimizing Decentralized Federated Learning
M. Pennisi, ..., and Marco Aldinucci. 2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), 2023. 1 citations.
1% Topic Match
Abstract: Federated and continual learning are training paradigms addressing data distribution shift in space and time. More specifically, federated learning tackles
non-i.i.d data in space as information is distributed in multiple nodes, while continual learning faces with temporal aspect of training as it deals with continuous
streams of data. Distribution shifts over space and time is what it happens in real federated learning scenarios that show multiple challenges. First, the federated
model needs to learn sequentially while retaining knowledge from the past training rounds. Second, the model has also to deal with concept drift from the distributed
data distributions. To address...

[25] Federated Continual Learning via Knowledge Fusion: A Survey
Xin Yang, ..., and Tianrui Li. IEEE Transactions on Knowledge and Data Engineering, 2023. 54 citations.
0% Topic Match
Abstract: Data privacy and silos are nontrivial and greatly challenging in many real-world applications. Federated learning is a decentralized approach to training
models across multiple local clients without the exchange of raw data from client devices to global servers. However, existing works focus on a static data environment
and ignore continual learning from streaming data with incremental tasks. Federated Continual Learning (FCL) is an emerging paradigm to address model learning
in both federated and continual learning environments. The key objective of FCL is to fuse heterogeneous knowledge from different clients and retain knowledge of
previous tasks while learning on new ones....

[26] Client-Side Adaptation to Concept Drift in Federated Learning
Finn Saile, ..., and Stefan Schulte. 2024 2nd International Conference on Federated Learning Technologies and Applications (FLTA), 2024. 2
citations.
0% Topic Match
Abstract: Federated Learning is a paradigm at the intersection of Machine Learning and Distributed Computing. The fundamental idea is that multiple agents
collaborate to train a common model without sharing their local data. In this work, we propose an algorithm that passively adapts to concept drift, the evolution of
data over time. Our algorithm is deployed on the client side. The main idea is to use a dynamic learning rate at each client which adapts automatically in presence
of concept drift. To calculate the learning rate we utilizes the individual training loss. We present an evaluation based on multiple datasets and...

[27] Streaming LifeLong Learning With Any-Time Inference
S. Banerjee, ..., and Vinay P. Namboodiri. 2023 IEEE International Conference on Robotics and Automation (ICRA), 2023. 3 citations.
0% Topic Match
Abstract: Despite rapid advancements in the lifelong learning (LL) research, a large body of research mainly focuses on improving the performance in the existing
static continual learning (CL) setups. These methods lack the ability to succeed in a rapidly changing dynamic environment, where an AI agent needs to quickly
learn new instances in a ‘single pass' from the non-i.i.d (also possibly temporally contiguous/coherent) data streams without suffering from catastrophic forgetting.
For practical applicability, we propose a novel lifelong learning approach, which is streaming, i.e., a single input sample arrives in each time step. Moreover, the
proposed approach is single pass, class-incremental,...

[28] Continual Federated Learning Based on Knowledge Distillation
Yuhang Ma, ..., and Lidan Shou. Unknown journal, 2022. 82 citations.
0% Topic Match
Abstract: Federated learning (FL) is a promising approach for learning a shared global model on decentralized data owned by multiple clients without exposing their
privacy. In real-world scenarios, data accumulated at the client-side varies in distribution over time. As a consequence, the global model tends to forget the knowledge
obtained from previous tasks while learning new tasks, showing signs of "catastrophic forgetting". Previous studies in centralized learning use techniques such as
data replay and parameter regularization to mitigate catastrophic forgetting. Unfortunately, these techniques cannot adequately solve the non-trivial problem in FL.
We propose Continual Federated Learning with Distillation (CFeD) to address...

[29] Incremental Learning in Online Scenario
Jiangpeng He, ..., and F. Zhu. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 153 citations.
0% Topic Match
Abstract: Modern deep learning approaches have achieved great success in many vision applications by training a model using all available task-specific data.
However, there are two major obstacles making it challenging to implement for real life applications: (1) Learning new classes makes the trained model quickly
forget old classes knowledge, which is referred to as catastrophic forgetting. (2) As new observations of old classes come sequentially over time, the distribution may
change in unforeseen way, making the performance degrade dramatically on future data, which is referred to as concept drift. Current state-of-the-art incremental
learning methods require a long time to train...

[30] Adaptive Streaming Continuous Learning System for Video Analytics
Tianyu Li, ..., and Yong Jiang. 2024 IEEE/ACM 32nd International Symposium on Quality of Service (IWQoS), 2024. 0 citations.
0% Topic Match
Abstract: Video analytics systems use deep learning models to perform inference on videos and are widely applied in fields such as smart cities and robotics.
However, in order to improve inference speed, models with fewer parameters are often chosen to deploy on edge devices, thereby sacrificing the detection accuracy
of the task. Continuous learning is an emerging approach to improve the accuracy of lightweight models deployed on the edge for video inference. However, most
solutions constantly upload data from edge to the cloud without considering the limited resources and latency of system modules, resulting in the unreliability of the
system and...

[31] Class-Incremental Experience Replay for Continual Learning under Concept Drift

View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 14/30

Undermind

REPORT CREATED ON
8/29/2025

Lukasz Korycki and B. Krawczyk. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2021. 33
citations.
0% Topic Match
Abstract: Modern machine learning systems need to be able to cope with constantly arriving and changing data. Two main areas of research dealing with such
scenarios are continual learning and data stream mining. Continual learning focuses on accumulating knowledge and avoiding forgetting, assuming information
once learned should be stored. Data stream mining focuses on adaptation to concept drift and discarding outdated information, assuming that only the most recent
data is relevant. While these two areas are mainly being developed in separation, they offer complementary views on the problem of learning from dynamic data.
There is a need for unifying them, by...

[32] Continual Learning with Gated Incremental Memories for sequential data processing
Andrea Cossu, ..., and D. Bacciu. 2020 International Joint Conference on Neural Networks (IJCNN), 2020. 22 citations.
0% Topic Match
Abstract: The ability to learn in dynamic, nonstationary environments without forgetting previous knowledge, also known as Continual Learning (CL), is a key
enabler for scalable and trustworthy deployments of adaptive solutions. While the importance of continual learning is largely acknowledged in machine vision and
reinforcement learning problems, this is mostly under-documented for sequence processing tasks. This work proposes a Recurrent Neural Network (RNN) model for
CL that is able to deal with concept drift in input distribution without forgetting previously acquired knowledge. We also implement and test a popular CL approach,
Elastic Weight Consolidation (EWC), on top of two different types...

[33] An Information-Theoretic Analysis for Federated Learning under Concept Drift
Fu Peng, ..., and Ming Tang. ArXiv, 2025. 1 citations.
0% Topic Match
Abstract: Recent studies in federated learning (FL) commonly train models on static datasets. However, real-world data often arrives as streams with shifting
distributions, causing performance degradation known as concept drift. This paper analyzes FL performance under concept drift using information theory and
proposes an algorithm to mitigate the performance degradation. We model concept drift as a Markov chain and introduce the \emph{Stationary Generalization
Error} to assess a model's capability to capture characteristics of future unseen data. Its upper bound is derived using KL divergence and mutual information. We
study three drift patterns (periodic, gradual, and random) and their impact on FL...

[34] Tackling Online One-Class Incremental Learning by Removing Negative Contrasts
Nader Asadi, ..., and Eugene Belilovsky. ArXiv, 2022. 5 citations.
0% Topic Match
Abstract: Recent work studies the supervised online continual learning setting where a learner receives a stream of data whose class distribution changes over
time. Distinct from other continual learning settings the learner is presented new samples only once and must distinguish between all seen classes. A number of
successful methods in this setting focus on storing and replaying a subset of samples alongside incoming data in a computationally efficient manner. One recent
proposal ER-AML achieved strong performance in this setting by applying an asymmetric loss based on contrastive learning to the incoming data and replayed
data. However, a key ingredient of...

[35] Combining Diverse Meta-Features to Accurately Identify Recurring Concept Drift in Data Streams
B. Halstead, ..., and A. Bifet. ACM Transactions on Knowledge Discovery from Data, 2023. 14 citations.
0% Topic Match
Abstract: Learning from streaming data is challenging as the distribution of incoming data may change over time, a phenomenon known as concept drift. The
predictive patterns, or experience learned under one distribution may become irrelevant as conditions change under concept drift, but may become relevant once
again when conditions reoccur. Adaptive learning methods adapt a classifier to concept drift by identifying which distribution, or concept, is currently present in order
to determine which experience is relevant. Identifying a concept requires some representation to be stored for comparison, with the quality of the representation
being key to accurate identification. Existing concept representations...

[36] Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment
Alif Ashrafee, ..., and B. Krawczyk. ArXiv, 2025. 0 citations.
0% Topic Match
Abstract: Traditional continual learning methods prioritize knowledge retention and focus primarily on mitigating catastrophic forgetting, implicitly assuming that the
data distribution of previously learned tasks remains static. This overlooks the dynamic nature of real-world data streams, where concept drift permanently alters
previously seen data and demands both stability and rapid adaptation. We introduce a holistic framework for continual learning under concept drift that simulates
realistic scenarios by evolving task distributions. As a baseline, we consider Full Relearning (FR), in which the model is retrained from scratch on newly labeled
samples from the drifted distribution. While effective, this approach incurs substantial annotation...

[37] Concept Drift Detection from Multi-Class Imbalanced Data Streams
Lukasz Korycki and B. Krawczyk. 2021 IEEE 37th International Conference on Data Engineering (ICDE), 2021. 43 citations.
0% Topic Match
Abstract: Continual learning from data streams is among the most important topics in contemporary machine learning. One of the biggest challenges in this domain
lies in creating algorithms that can continuously adapt to arriving data. However, previously learned knowledge may become outdated, as streams evolve over
time. This phenomenon is known as concept drift and must be detected to facilitate efficient adaptation of the learning model. While there exists a plethora of drift
detectors, all of them assume that we are dealing with roughly balanced classes. In the case of imbalanced data streams, those detectors will be biased towards
the majority...

[38] Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams
Matthias De Lange and T. Tuytelaars. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 2020. 211 citations.
0% Topic Match
Abstract: Attaining prototypical features to represent class distributions is well established in representation learning. However, learning prototypes online from
streaming data proves a challenging endeavor as they rapidly become outdated, caused by an ever-changing parameter space during the learning process.
Additionally, continual learning does not assume the data stream to be stationary, typically resulting in catastrophic forgetting of previous knowledge. As a first,
we introduce a system addressing both problems, where prototypes evolve continually in a shared latent space, enabling learning and prediction at any point in
time. To facilitate learning, a novel objective function synchronizes the latent space with the...

[39] DriftSurf: Stable-State / Reactive-State Learning under Concept Drift
Ashraf Tahmasbi, ..., and Phillip B. Gibbons. Unknown journal, 2021. 42 citations.
0% Topic Match
No summary or abstract available

[40] Continual Learning Approach for Continuous Data Stream Analysis in Dynamic Environments
K. Prasanna, ..., and J. Chinna Babu. Applied Sciences, 2023. 8 citations.
0% Topic Match
Abstract: Continuous data stream analysis primarily focuses on the unanticipated changes in the transmission of data distribution over time. Conceptual change
is defined as the signal distribution changes over the transmission of continuous data streams. A drift detection scenario is set forth to develop methods and
strategies for detecting, interpreting, and adapting to conceptual changes over data streams. Machine learning approaches can produce poor learning outcomes in
the conceptual change environment if the sudden change is not addressed. Furthermore, due to developments in concept drift, learning methodologies have been
significantly systematic in recent years. The research introduces a novel approach using...

[41] Tracking changes using Kullback-Leibler divergence for the continual learning
Sebastian Basterrech and Michal Wo'zniak. 2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 2022. 14 citations.
0% Topic Match
Abstract: Recently, continual learning has received a lot of attention. One of the significant problems is the occurrence of concept drift, which consists of changing
probabilistic characteristics of the incoming data. In the case of the classification task, this phenomenon destabilizes the model’s performance and negatively affects

View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 15/30

Undermind

REPORT CREATED ON
8/29/2025

the achieved prediction quality. Most current methods apply statistical learning and similarity analysis over the raw data. However, similarity analysis in streaming
data remains a complex problem due to time limitation, non-precise values, fast decision speed, scalability, etc. This article introduces a novel method for monitoring
changes in the probabilistic distribution of multi-dimensional data...

[42] Continual Learning with Deep Streaming Regularized Discriminant Analysis
Joe Khawand, ..., and David Colliaux. 2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), 2023. 1 citations.
0% Topic Match
Abstract: Continual learning is increasingly sought after in real-world machine learning applications, as it enables learning in a more human-like manner. Conventional
machine learning approaches fail to achieve this, as incrementally updating the model with non-identically distributed data leads to catastrophic forgetting, where
existing representations are overwritten. Although traditional continual learning methods have mostly focused on batch learning, which involves learning from large
collections of labeled data sequentially, this approach is not well-suited for real-world applications where we would like new data to be integrated directly. This
necessitates a paradigm shift towards streaming learning. In this paper, we propose1 a streaming...

[43] StreamMLOps: Operationalizing Online Learning for Big Data Streaming & Real-Time Applications
Mariam Barry, ..., and Eric Guerizec. 2023 IEEE 39th International Conference on Data Engineering (ICDE), 2023. 8 citations.
0% Topic Match
Abstract: Continuously learning and serving from evolving streaming data and serving in real-time is a challenging problem. Traditionally, data is partitioned and
processed in batches to train machine learning (ML) models. In industrial applications, static models’ performance drops over time (model degradation, concept
drift), requiring new models to be trained with recent data and redeployed in production. The scientific community has been studying online and adaptive methods
to address batch-learning limitations and continuously train AI tasks for industrial applications such as cyber-security, AIOps, anomaly scoring, and drift detection
in stock markets. This paper deals with the MLOps aspects of deploying such...

[44] Dark Experience for General Continual Learning: a Strong, Simple Baseline
Pietro Buzzega, ..., and S. Calderara. ArXiv, 2020. 983 citations.
0% Topic Match
Abstract: Neural networks struggle to learn continuously, as they forget the old knowledge catastrophically whenever the data distribution changes over time. Recently,
Continual Learning has inspired a plethora of approaches and evaluation settings; however, the majority of them overlooks the properties of a practical scenario,
where the data stream cannot be shaped as a sequence of tasks and offline training is not viable. We work towards General Continual Learning (GCL), where
task boundaries blur and the domain and class distributions shift either gradually or suddenly. We address it through Dark Experience Replay, namely matching the
network's logits sampled throughout the optimization...

[45] Class Incremental Online Streaming Learning
S. Banerjee, ..., and Vinay P. Namboodiri. ArXiv, 2021. 4 citations.
0% Topic Match
Abstract: A wide variety of methods have been developed to enable lifelong learning in conventional deep neural networks. However, to succeed, these methods
require a `batch' of samples to be available and visited multiple times during training. While this works well in a static setting, these methods continue to suffer in a
more realistic situation where data arrives in \emph{online streaming manner}. We empirically demonstrate that the performance of current approaches degrades
if the input is obtained as a stream of data with the following restrictions: $(i)$ each instance comes one at a time and can be seen only once, and...

[46] Recent Advances in Concept Drift Adaptation Methods for Deep Learning
Liheng Yuan, ..., and Xinge You. Unknown journal, 2022. 31 citations.
0% Topic Match
Abstract: In the ``Big Data'' age, the amount and distribution of data have increased wildly and changed over time in various time-series-based tasks, e.g weather
prediction, network intrusion detection. However, deep learning models may become outdated facing variable input data distribution, which is called concept drift. To
address this problem, large number of samples are usually required to update deep learning models, which is impractical in many realistic applications. This challenge
drives researchers to explore the effective ways to adapt deep learning models to concept drift. In this paper, we first mathematically describe the categories of
concept drift including abrupt drift,...

[47] Multilayer Neuromodulated Architectures for Memory-Constrained Online Continual Learning
Sandeep Madireddy, ..., and Prasanna Balaprakash. ArXiv, 2020. 0 citations.
0% Topic Match
Abstract: We focus on the problem of how to achieve online continual learning under memory-constrained conditions where the input data may not be known a priori.
These constraints are relevant in edge computing scenarios. We have developed an architecture where input processing over data streams and online learning
are integrated in a single recurrent network architecture. This allows us to cast metalearning optimization as a mixed-integer optimization problem, where different
synaptic plasticity algorithms and feature extraction layers can be swapped out and their hyperparameters are optimized to identify optimal architectures for different
sets of tasks. We utilize a Bayesian optimization method...

[48] Improving Performance in Continual Learning Tasks using Bio-Inspired Architectures
Sandeep Madireddy, ..., and Prasanna Balaprakash. ArXiv, 2023. 2 citations.
0% Topic Match
Abstract: The ability to learn continuously from an incoming data stream without catastrophic forgetting is critical to designing intelligent systems. Many approaches
to continual learning rely on stochastic gradient descent and its variants that employ global error updates, and hence need to adopt strategies such as memory
buffers or replay to circumvent its stability, greed, and short-term memory limitations. To address this limitation, we have developed a biologically inspired lightweight
neural network architecture that incorporates synaptic plasticity mechanisms and neuromodulation and hence learns through local error signals to enable online
continual learning without stochastic gradient descent. Our approach leads to superior...

[49] Kullback-Leibler Reservoir Sampling for Fairness in Continual Learning
Sotirios Nikoloutsopoulos, ..., and Michalis K. Titsias. 2024 IEEE International Conference on Machine Learning for Communication and
Networking (ICMLCN), 2024. 1 citations.
0% Topic Match
Abstract: Continual Learning (CL) aims to update a ML model with continuously arriving training data without having to retrain the model again from scratch. Continual
Learning will play a critical role in future AI-enabled services at the network edge. We propose an online replay-based Continual Learning method for classification
problems, in which the learner stores data points to a buffer and replays them during training. The core of our contribution is a new replay buffer content update
policy that selects data points to store in the buffer by minimizing the Kullback-Leibler (KL) loss between the buffer distribution and a target distribution....

[50] Online Learning on Non-Stationary Data Streams for Image Recognition using Deep Embeddings
Valerie Vaquet, ..., and Barbara Hammer. 2021 IEEE Symposium Series on Computational Intelligence (SSCI), 2021. 3 citations.
0% Topic Match
Abstract: Deep neural networks offer state-of-the-art technologies for highly nonlinear domains such as image processing; yet their initial training requires large
amounts of data, such that they are not directly suited for online learning scenarios for streaming data where class distributions or class labels may change over
time. In this contribution, we investigate the suitability of a combination of recent online learning technologies, which have been proposed for learning with streaming
data and concept drift in simpler settings, and deep representations of image data as provided by deep networks trained in batch mode, to offer flexible learning
technologies for streaming data...

View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 16/30

Undermind

REPORT CREATED ON
8/29/2025

[51] Bypassing Logits Bias in Online Class-Incremental Learning with a Generative Framework
Gehui Shen, ..., and Zhi-Hong Deng. ArXiv, 2022. 0 citations.
0% Topic Match
Abstract: Continual learning requires the model to maintain the learned knowledge while learning from a non-i.i.d data stream continually. Due to the single-pass
training setting, online continual learning is very challenging, but it is closer to the real-world scenarios where quick adaptation to new data is appealing. In this
paper, we focus on online class-incremental learning setting in which new classes emerge over time. Almost all existing methods are replay-based with a softmax
classifier. However, the inherent logits bias problem in the softmax classifier is a main cause of catastrophic forgetting while existing solutions are not applicable for
online settings. To...

[52] New Insights on Reducing Abrupt Representation Change in Online Continual Learning
Lucas Caccia, ..., and Eugene Belilovsky. ArXiv, 2021. 220 citations.
0% Topic Match
Abstract: In the online continual learning paradigm, agents must learn from a changing distribution while respecting memory and compute constraints. Experience
Replay (ER), where a small subset of past data is stored and replayed alongside new data, has emerged as a simple and effective learning strategy. In this work,
we focus on the change in representations of observed data that arises when previously unobserved classes appear in the incoming data stream, and new classes
must be distinguished from previous ones. We shed new light on this question by showing that applying ER causes the newly added classes' representations to
overlap significantly...

[53] Clustering-based Domain-Incremental Learning
Christiaan Lamers, ..., and Paris Giampouras. 2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), 2023. 12
citations.
0% Topic Match
Abstract: We consider the problem of learning multiple tasks in a continual learning setting in which data from different tasks is presented to the learner in a streaming
fashion. A key challenge in this setting is the so-called "catastrophic forgetting problem", in which the performance of the learner in an "old task" decreases when
subsequently trained on a "new task". Existing continual learning methods, such as Averaged Gradient Episodic Memory (A-GEM) and Orthogonal Gradient Descent
(OGD), address catastrophic forgetting by minimizing the loss for the current task without increasing the loss for previous tasks. However, these methods assume
the learner knows...

[54] A Simple Baseline for Stable and Plastic Neural Networks
Etienne Künzel, ..., and Visvanathan Ramesh. ArXiv, 2025. 0 citations.
0% Topic Match
Abstract: Continual learning in computer vision requires that models adapt to a continuous stream of tasks without forgetting prior knowledge, yet existing approaches
often tip the balance heavily toward either plasticity or stability. We introduce RDBP, a simple, low-overhead baseline that unites two complementary mechanisms:
ReLUDown, a lightweight activation modification that preserves feature sensitivity while preventing neuron dormancy, and Decreasing Backpropagation, a
biologically inspired gradient-scheduling scheme that progressively shields early layers from catastrophic updates. Evaluated on the Continual ImageNet benchmark,
RDBP matches or exceeds the plasticity and stability of state-of-the-art methods while reducing computational cost. RDBP thus provides both a practical...

[55] Streaming learning with Move-to-Data approach for image classification
Abel Kahsay Gebreslassie, ..., and A. Zemmari. Proceedings of the 19th International Conference on Content-based Multimedia Indexing, 2022.
1 citations.
0% Topic Match
Abstract: In Deep Neural Network training, the availability of a large amount of representative training data is the sine qua non-condition for a good generalization
capacity of the model. In many real-world applications, data is not available at a glance, but coming on the fly. If a pre-trained model is fine-tuned on the new data,
then catastrophic forgetting happens mostly. Incremental learning mechanisms propose ways to overcome catastrophic forgetting. Streaming learning is a type of
incremental learning where models learn from new data instances as soon as they become available in a single training pass. In this work, we conduct an...

[56] Mitigating Forgetting in Online Continual Learning via Instance-Aware Parameterization
Hung-Jen Chen, ..., and Min Sun. Unknown journal, 2020. 43 citations.
0% Topic Match
No summary or abstract available

[57] Online Fast Adaptation and Knowledge Accumulation (OSAKA): a New Approach to Continual Learning
Massimo Caccia, ..., and Laurent Charlin. Unknown journal, 2020. 72 citations.
0% Topic Match
No summary or abstract available

[58] Unsupervised Unlearning of Concept Drift with Autoencoders
André Artelt, ..., and Barbara Hammer. 2023 IEEE Symposium Series on Computational Intelligence (SSCI), 2022. 4 citations.
0% Topic Match
Abstract: Concept drift refers to a change in the data distribution affecting the data stream of future samples. Consequently, learning models operating on the data
stream might become obsolete, and need costly and difficult adjustments such as retraining or adaptation. Existing methods usually implement a local concept drift
adaptation scheme, where either incremental learning of the models is used, or the models are completely retrained when a drift detection mechanism triggers an
alarm. This paper proposes an alternative approach in which an unsupervised and model-agnostic concept drift adaptation method at the global level is introduced,
based on autoencoders. Specifically, the proposed...

[59] Lean and Mean Adaptive Optimization via Subset-Norm and Subspace-Momentum with Convergence Guarantees
Thien Hang Nguyen and Huy Nguyen. ArXiv, 2024. 0 citations.
0% Topic Match
Abstract: We introduce two complementary techniques for efficient optimization that reduce memory requirements while accelerating training of large-scale neural
networks. The first technique, Subset-Norm step size, generalizes AdaGrad-Norm and AdaGrad(-Coordinate) through step-size sharing. Subset-Norm (SN) reduces
AdaGrad's memory footprint from $O(d)$ to $O(\sqrt{d})$, where $d$ is the model size. For non-convex smooth objectives under coordinate-wise sub-gaussian noise,
we show a noise-adapted high-probability convergence guarantee with improved dimensional dependence of SN over existing methods. Our second technique,
Subspace-Momentum, reduces the momentum state's memory footprint by restricting momentum to a low-dimensional subspace while performing SGD in the
orthogonal complement. We prove a high-probability...

[60] Online Continual Learning with Maximally Interfered Retrieval
Rahaf Aljundi, ..., and T. Tuytelaars. ArXiv, 2019. 567 citations.
0% Topic Match
Abstract: Continual learning, the setting where a learning agent is faced with a never ending stream of data, continues to be a great challenge for modern machine
learning systems. In particular the online or "single-pass through the data" setting has gained attention recently as a natural setting that is difficult to tackle. Methods
based on replay, either generative or from a stored memory, have been shown to be effective approaches for continual learning, matching or exceeding the state of
the art in a number of standard benchmarks. These approaches typically rely on randomly selecting samples from the replay memory or from...

View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 17/30

Undermind

REPORT CREATED ON
8/29/2025

[61] Is Class-Incremental Enough for Continual Learning?
Andrea Cossu, ..., and Vincenzo Lomonaco. Frontiers in Artificial Intelligence, 2021. 33 citations.
0% Topic Match
Abstract: The ability of a model to learn continually can be empirically assessed in different continual learning scenarios. Each scenario defines the constraints and
the opportunities of the learning environment. Here, we challenge the current trend in the continual learning literature to experiment mainly on class-incremental
scenarios, where classes present in one experience are never revisited. We posit that an excessive focus on this setting may be limiting for future research on
continual learning, since class-incremental scenarios artificially exacerbate catastrophic forgetting, at the expense of other important objectives like forward transfer
and computational efficiency. In many real-world environments, in fact, repetition...

[62] Always Strengthen Your Strengths: A Drift-Aware Incremental Learning Framework for CTR Prediction
Congcong Liu, ..., and Jingping Shao. Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information
Retrieval, 2023. 7 citations.
0% Topic Match
Abstract: CTR prediction is crucial in recommendation systems and online advertising platforms, where user-generated data streams that drift over time can lead to
catastrophic forgetting if the model continuously adapts to new data distribution. Conventional strategies for catastrophic forgetting are challenging to deploy due to
memory constraints and diverse data distributions. To address this, we propose a novel drift-aware incremental learning framework based on ensemble learning for
CTR prediction, which uses explicit error-based drift detection on streaming data to strengthen well-adapted ensembles and freeze ensembles that do not match
the input distribution, avoiding catastrophic interference. Our method outperforms all baselines considered...

[63] An Impact Study of Concept Drift in Federated Learning
Guanhui Yang, ..., and Yun Yang. 2023 IEEE International Conference on Data Mining (ICDM), 2023. 7 citations.
0% Topic Match
Abstract: Federated learning (FL) is a rising distributed machine learning area, which aims to train a high-performing global model with data collected from a number
of local clients. Many FL applications receive data over time in the form of data streams. Streaming data are likely to suffer concept drift. It can significantly harm a
model’s predictive ability. However, no study has characterized concept drift in FL or investigated how it can affect the global and local models’ performance. This
paper aims to provide such understanding by 1) categorizing concept drift in temporal and spatial dimensions with ten features and 2) investigating...

[64] Online Drift Detection with Maximum Concept Discrepancy
Ke Wan, ..., and Susik Yoon. Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2024. 6 citations.
0% Topic Match
Abstract: Continuous learning from an immense volume of data streams becomes exceptionally critical in the internet era. However, data streams often do not
conform to the same distribution over time, leading to a phenomenon called concept drift. Since a fixed static model is unreliable for inferring concept-drifted data
streams, establishing an adaptive mechanism for detecting concept drift is crucial. Current methods for concept drift detection primarily assume that the labels
or error rates of downstream models are given and/or underlying statistical properties exist in data streams. These approaches, however, struggle to address
high-dimensional data streams with intricate irregular distribution shifts, which...

[65] Gradient based sample selection for online continual learning
Rahaf Aljundi, ..., and Yoshua Bengio. Unknown journal, 2019. 868 citations.
0% Topic Match
Abstract: A continual learning agent learns online with a non-stationary and never-ending stream of data. The key to such learning process is to overcome the
catastrophic forgetting of previously seen data, which is a well known problem of neural networks. To prevent forgetting, a replay buffer is usually employed to store
the previous data for the purpose of rehearsal. Previous works often depend on task boundary and i.i.d. assumptions to properly select samples for the replay buffer.
In this work, we formulate sample selection as a constraint reduction problem based on the constrained optimization view of continual learning. The goal is...

[66] Liquid Neural Network-based Adaptive Learning vs. Incremental Learning for Link Load Prediction amid Concept Drift due to Network Failures
Omran Ayoub, ..., and Krzysztof Walkowiak. ArXiv, 2024. 3 citations.
0% Topic Match
Abstract: Adapting to concept drift is a challenging task in machine learning, which is usually tackled using incremental learning techniques that periodically re-fit a
learning model leveraging newly available data. A primary limitation of these techniques is their reliance on substantial amounts of data for retraining. The necessity
of acquiring fresh data introduces temporal delays prior to retraining, potentially rendering the models inaccurate if a sudden concept drift occurs in-between two
consecutive retrainings. In communication networks, such issue emerges when performing traffic forecasting following a~failure event: post-failure re-routing may
induce a drastic shift in distribution and pattern of traffic data, thus...

[67] A Scalable Approach to Covariate and Concept Drift Management via Adaptive Data Segmentation
Vennela Yarabolu, ..., and Siddhartha Asthana. Proceedings of the 8th International Conference on Data Science and Management of Data (12th
ACM IKDD CODS and 30th COMAD), 2024. 0 citations.
0% Topic Match
Abstract: In many real-world applications, continuous machine learning (ML) systems are crucial but prone to data drift—a phenomenon where discrepancies
between historical training data and future test data lead to significant performance degradation and operational inefficiencies. Traditional drift adaptation methods
typically update models using ensemble techniques, often discarding drifted historical data, and focus primarily on either covariate drift or concept drift. These
methods face issues such as high resource demands, inability to manage all types of drifts effectively, and neglecting the valuable context that historical data can
provide. We contend that explicitly incorporating drifted data into the model training process significantly...

[68] AEDFL: Efficient Asynchronous Decentralized Federated Learning with Heterogeneous Devices
Ji Liu, ..., and P. Valduriez. ArXiv, 2023. 15 citations.
0% Topic Match
Abstract: Federated Learning (FL) has achieved significant achievements recently, enabling collaborative model training on distributed data over edge devices.
Iterative gradient or model exchanges between devices and the centralized server in the standard FL paradigm suffer from severe efficiency bottlenecks on the server.
While enabling collaborative training without a central server, existing decentralized FL approaches either focus on the synchronous mechanism that deteriorates
FL convergence or ignore device staleness with an asynchronous mechanism, resulting in inferior FL accuracy. In this paper, we propose an Asynchronous Efficient
Decentralized FL framework, i.e., AEDFL, in heterogeneous environments with three unique contributions. First, we propose...

[69] DriftSurf: A Risk-competitive Learning Algorithm under Concept Drift
Ashraf Tahmasbi, ..., and Phillip B. Gibbons. ArXiv, 2020. 11 citations.
0% Topic Match
Abstract: When learning from streaming data, a change in the data distribution, also known as concept drift, can render a previously-learned model inaccurate and
require training a new model. We present an adaptive learning algorithm that extends previous drift-detection-based methods by incorporating drift detection into
a broader stable-state/reactive-state process. The advantage of our approach is that we can use aggressive drift detection in the stable state to achieve a high
detection rate, but mitigate the false positive rate of standalone drift detection via a reactive state that reacts quickly to true drifts while eliminating most false
positives. The algorithm is generic...

[70] Offloading Strategy for Forest Monitoring Network Based on Improved Beetle Optimization Algorithm
Xiaohui Cheng, ..., and Junyu Zhao. Symmetry, 2024. 1 citations.
0% Topic Match
Abstract: In forest monitoring networks, the computational capabilities of sensors cannot meet the latency requirements for complex tasks, and the limited battery
capacity of these sensors hinders the long-term execution of monitoring tasks. Mobile edge computing (MEC) acts as an effective solution for this issue by offloading
tasks to edge servers, significantly reducing both task latency and energy consumption. However, the computational capacity of MEC servers and the bandwidth in
the system are limited, and the communication environment in forested areas is complex. To simulate the complexity of the forest communication environment, we
incorporate empirical path loss and multipath fading into...

[71] A Lightweight Concept Drift Detection and Adaptation Framework for IoT Data Streams
View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 18/30

Undermind

REPORT CREATED ON
8/29/2025

Li Yang and A. Shami. IEEE Internet of Things Magazine, 2021. 111 citations.
0% Topic Match
Abstract: In recent years, with the increasing popularity of “Smart Technology”, the number of Internet of Things (IoT) devices and systems have surged significantly.
Various IoT services and functionalities are based on the analytics of IoT streaming data. However, IoT data analytics faces concept drift challenges due to the
dynamic nature of IoT systems and the ever-changing patterns of IoT data streams. In this article, we propose an adaptive IoT streaming data analytics framework
for anomaly detection use cases based on optimized LightGBM and concept drift adaptation. A novel drift adaptation method named Optimized Adaptive and Sliding
Windowing (OASW) is proposed...

[72] Adapting to Online Distribution Shifts in Deep Learning: A Black-Box Approach
Dheeraj Baby, ..., and Yu-Xiang Wang. Unknown journal, 2025. 1 citations.
0% Topic Match
Abstract: We study the well-motivated problem of online distribution shift in which the data arrive in batches and the distribution of each batch can change arbitrarily
over time. Since the shifts can be large or small, abrupt or gradual, the length of the relevant historical data to learn from may vary over time, which poses a major
challenge in designing algorithms that can automatically adapt to the best ``attention span'' while remaining computationally efficient. We propose a meta-algorithm
that takes any network architecture and any Online Learner (OL) algorithm as input and produces a new algorithm which provably enhances the performance...

[73] Scalable and Fully Configurable NoC-based Hardware Implemention of Growing Neural Gas for Continual Learning
F. Derue, ..., and Serge Weber. 2024 31st IEEE International Conference on Electronics, Circuits and Systems (ICECS), 2024. 0 citations.
0% Topic Match
Abstract: Due to their accuracy and performances, neural networks (NN), like deep learning, are preferred in many application fields, but face challenges such as
concept drift, requiring adaptable models for evolving data distributions. Concepts such as Continual (CL) or Incremental Learning (IL) are proposed as solutions
to handle data changes over time and prevent catastrophic forgetting. Prototype-based methods have shown promise in addressing these challenges, offering
robustness to adversarial attacks and interpretability. In this paper, we focus on the hardware implementation of one of the prototype-based methods, namely
Growing Neural Gas, which is a dynamically evolving NN suited for CL. Its...

[74] ADDAEIL: Anomaly Detection with Drift-Aware Ensemble-Based Incremental Learning
Danlei Li, ..., and Kevin I.-Kai Wang. Algorithms, 2025. 0 citations.
0% Topic Match
Abstract: Time series anomaly detection in streaming environments faces persistent challenges due to concept drift, which gradually degrades model reliability. In
this paper, we propose Anomaly Detection with Drift-Aware Ensemble-based Incremental Learning (ADDAEIL), an unsupervised anomaly detection framework that
incrementally adapts to concept drift in non-stationary streaming time series data. ADDAEIL integrates a hybrid drift detection mechanism that combines statistical
distribution tests with structural-based performance evaluation of base detectors in Isolation Forest. This design enables unsupervised detection and continuous
adaptation to evolving data patterns. Based on the estimated drift intensity, an adaptive update strategy selectively replaces degraded base detectors. This allows...

[75] Matchmaker: Data Drift Mitigation in Machine Learning for Large-Scale Systems
Ankur Mallick, ..., and Gauri Joshi. Conference on Machine Learning and Systems, 2022. 64 citations.
0% Topic Match
No summary or abstract available

[76] iTAML: An Incremental Task-Agnostic Meta-learning Approach
Jathushan Rajasegaran, ..., and M. Shah. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 157 citations.
0% Topic Match
Abstract: Humans can continuously learn new knowledge as their experience grows. In contrast, previous learning in deep neural networks can quickly fade out
when they are trained on a new task. In this paper, we hypothesize this problem can be avoided by learning a set of generalized parameters, that are neither specific
to old nor new tasks. In this pursuit, we introduce a novel meta-learning approach that seeks to maintain an equilibrium between all the encountered tasks. This is
ensured by a new meta-update rule which avoids catastrophic forgetting. In comparison to previous meta-learning techniques, our approach is task-agnostic. When
presented...

[77] Online Deep Learning: Learning Deep Neural Networks on the Fly
Doyen Sahoo, ..., and S. Hoi. ArXiv, 2017. 322 citations.
0% Topic Match
Abstract: Deep Neural Networks (DNNs) are typically trained by backpropagation in a batch setting, requiring the entire training data to be made available prior to the
learning task. This is not scalable for many real-world scenarios where new data arrives sequentially in a stream. We aim to address an open challenge of ``Online
Deep Learning" (ODL) for learning DNNs on the fly in an online setting. Unlike traditional online learning that often optimizes some convex objective function with
respect to a shallow model (e.g., a linear/kernel-based hypothesis), ODL is more challenging as the optimization objective is non-convex, and regular DNN with...

[78] Non-IID data and Continual Learning processes in Federated Learning: A long road ahead
Marcos F. Criado, ..., and S. Barro. Inf. Fusion, 2021. 91 citations.
0% Topic Match
No summary or abstract available

[79] Learning a Unified Classifier Incrementally via Rebalancing
Saihui Hou, ..., and Dahua Lin. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 1186 citations.
0% Topic Match
Abstract: Conventionally, deep neural networks are trained offline, relying on a large dataset prepared in advance. This paradigm is often challenged in real-world
applications, e.g. online services that involve continuous streams of incoming data. Recently, incremental learning receives increasing attention, and is considered
as a promising solution to the practical challenges mentioned above. However, it has been observed that incremental learning is subject to a fundamental difficulty
-- catastrophic forgetting, namely adapting a model to new data often results in severe performance degradation on previous tasks or classes. Our study reveals
that the imbalance between previous and new data is a...

[80] DriftMoE: A Mixture of Experts Approach to Handle Concept Drifts
Miguel Aspis, ..., and Ricardo Sim'on Carbajo. ArXiv, 2025. 0 citations.
0% Topic Match
Abstract: Learning from non-stationary data streams subject to concept drift requires models that can adapt on-the-fly while remaining resource-efficient. Existing
adaptive ensemble methods often rely on coarse-grained adaptation mechanisms or simple voting schemes that fail to optimally leverage specialized knowledge.
This paper introduces DriftMoE, an online Mixture-of-Experts (MoE) architecture that addresses these limitations through a novel co-training framework. DriftMoE
features a compact neural router that is co-trained alongside a pool of incremental Hoeffding tree experts. The key innovation lies in a symbiotic learning loop that
enables expert specialization: the router selects the most suitable expert for prediction, the relevant experts update...

[81] Learning Convolutional Neural Networks in presence of Concept Drift
Simone Disabato and M. Roveri. 2019 International Joint Conference on Neural Networks (IJCNN), 2019. 19 citations.
0% Topic Match
Abstract: Designing adaptive machine learning systems able to operate in nonstationary conditions, also called concept drift, is a novel and promising research
area. Convolutional Neural Networks (CNNs) have not been considered a viable solution for such adaptive systems due to the high computational load and the
high number of images they require for the training. This paper introduces an adaptive mechanism for learning CNNs able to operate in presence of concept drift.
Such an adaptive mechanism follows an "active approach", where the adaptation is triggered by the detection of a concept drift, and relies on the "transfer learning"
paradigm to transfer...

[82] Class-incremental Learning via Deep Model Consolidation
Junting Zhang, ..., and C.-C. Jay Kuo. 2020 IEEE Winter Conference on Applications of Computer Vision (WACV), 2019. 347 citations.
0% Topic Match
View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 19/30

Undermind

REPORT CREATED ON
8/29/2025

Abstract: Deep neural networks (DNNs) often suffer from "catastrophic forgetting" during incremental learning (IL) — an abrupt degradation of performance on the
original set of classes when the training objective is adapted to a newly added set of classes. Existing IL approaches tend to produce a model that is biased towards
either the old classes or new classes, unless with the help of exemplars of the old data. To address this issue, we propose a class-incremental learning paradigm
called Deep Model Consolidation (DMC), which works well even when the original training data is not available. The idea is to first train...

[83] Embracing Change: Continual Learning in Deep Neural Networks
R. Hadsell, ..., and Razvan Pascanu. Trends in Cognitive Sciences, 2020. 515 citations.
0% Topic Match
No summary or abstract available

[84] Fast Learning and Testing for Imbalanced Multi-Class Changes in Streaming Data by Dynamic Multi-Stratum Network
Mongkhon Thakong, ..., and C. Lursinsap. IEEE Access, 2017. 6 citations.
0% Topic Match
Abstract: Although several efficient learning methods have recently been proposed to handle class drift situations, issues remain in various streaming data
applications that possibly deteriorate classification accuracy. Three important issues were considered, that is: 1) lifetime and class changes; 2) high imbalance
ratios of streaming data among classes; and 3) classification accuracy of untrained data and class-changed data. A new dynamical learning structure based on
hyper-elliptical capsule and multi-stratum network was introduced to cope with these issues. The experimental results on a simulated University of California at
Irvine non-concept-drift database and real concept-drift data confirm that the proposed multi-stratum learning provided...

[85] Online Continual Learning in Image Classification: An Empirical Survey
Zheda Mai, ..., and S. Sanner. ArXiv, 2021. 440 citations.
0% Topic Match
No summary or abstract available

[86] Dedicated Memory Models for Continual Learning in the Presence of Concept Drift
Viktor Losing, ..., and H. Wersing. Unknown journal, 2016. 1 citations.
0% Topic Match
No summary or abstract available

[87] Towards Building a Flexible Online Learning Model for Data Stream Classification
Deeksha Aggarwal, ..., and J. Senthilnath. Proceedings of the 5th Joint International Conference on Data Science & Management of Data (9th
ACM IKDD CODS and 27th COMAD), 2022. 1 citations.
0% Topic Match
Abstract: In the current era, data are generated by almost every electronic devices. These data are often produced continuously from many IoT devices resulting in an
enormous corpus of data streams. To learn and model such data streams, an adaptive, robust and scalable learning algorithm is needed which can handle concept
drift and also able to retain previously learned knowledge. In real-world applications, labeling every data item in data streams is time and resource consuming.
Therefore, in this work, we propose a self-evolving online learning algorithm that not only efficiently learn the data streams under limited labeled data conditions but
also...

[88] Avalanche: an End-to-End Library for Continual Learning
Vincenzo Lomonaco, ..., and D. Maltoni. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2021.
191 citations.
0% Topic Match
Abstract: Learning continually from non-stationary data streams is a long-standing goal and a challenging problem in machine learning. Recently, we have witnessed
a renewed and fast-growing interest in continual learning, especially within the deep learning community. However, algorithmic solutions are often difficult to
re-implement, evaluate and port across different settings, where even results on standard benchmarks are hard to reproduce. In this work, we propose Avalanche,
an open-source end-to-end library for continual learning research based on PyTorch. Avalanche is designed to provide a shared and collaborative codebase for fast
prototyping, training, and reproducible evaluation of continual learning algorithms.

[89] Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting
Yuyang Liu, ..., and Yonghong Tian. ArXiv, 2025. 0 citations.
0% Topic Match
Abstract: Vision-language models (VLMs) have achieved impressive performance across diverse multimodal tasks by leveraging large-scale pre-training. However,
enabling them to learn continually from non-stationary data remains a major challenge, as their cross-modal alignment and generalization capabilities are particularly
vulnerable to catastrophic forgetting. Unlike traditional unimodal continual learning (CL), VLMs face unique challenges such as cross-modal feature drift, parameter
interference due to shared architectures, and zero-shot capability erosion. This survey offers the first focused and systematic review of continual learning for VLMs
(VLM-CL). We begin by identifying the three core failure modes that degrade performance in VLM-CL. Based on these, we propose...

[90] The Importance of Being Lazy: Scaling Limits of Continual Learning
Jacopo Graldi, ..., and Lorenzo Noci. ArXiv, 2025. 0 citations.
0% Topic Match
Abstract: Despite recent efforts, neural networks still struggle to learn in non-stationary environments, and our understanding of catastrophic forgetting (CF) is far
from complete. In this work, we perform a systematic study on the impact of model scale and the degree of feature learning in continual learning. We reconcile
existing contradictory observations on scale in the literature, by differentiating between lazy and rich training regimes through a variable parameterization of the
architecture. We show that increasing model width is only beneficial when it reduces the amount of feature learning, yielding more laziness. Using the framework of
dynamical mean field theory, we...

[91] Hierarchical Cluster-Based Adaptive Model for Semi-Supervised Classification of Data Stream with Concept Drift
Keke Qin and Yixiu Qin. Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science, 2019. 1 citations.
0% Topic Match
Abstract: Compared with the research of data stream with concept drift in supervised environment, the work that in semi-supervised environment is more challenging.
There is currently very little work in this area, although more meaningful. Considering existing chunk-based processing algorithms are only suitable for periodic
concept drift and show poor performance in complex concept drift scenarios, such as concept drift may occur at any time, the duration of each concept is not exactly
the same and multiple different types of concept drift types may alternate or mixed at the time. In this work, we propose an online plus offline memory model...

[92] Re-evaluating Continual Learning Scenarios: A Categorization and Case for Strong Baselines
Yen-Chang Hsu, ..., and Z. Kira. ArXiv, 2018. 367 citations.
0% Topic Match
Abstract: Continual learning has received a great deal of attention recently with several approaches being proposed. However, evaluations involve a diverse set of
scenarios making meaningful comparison difficult. This work provides a systematic categorization of the scenarios and evaluates them within a consistent framework
including strong baselines and state-of-the-art methods. The results provide an understanding of the relative difficulty of the scenarios and that simple baselines
(Adagrad, L2 regularization, and naive rehearsal strategies) can surprisingly achieve similar performance to current mainstream methods. We conclude with several
suggestions for creating harder evaluation scenarios and future research directions. The code is available at...

[93] Federated Learning with Partial Gradients Over-the-Air
Wendi Wang, ..., and Howard H. Yang. 2023 20th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON),
2023. 1 citations.
0% Topic Match
Abstract: We develop a theoretical framework to study the training of federated learning models with partial gradients via over-the-air computing. The system consists
of an edge server and multiple clients, aiming to collaboratively minimize a global loss function. The clients conduct local training and upload the intermediate
parameters (e.g. the gradients) by analog transmissions. Specifically, each client modulates the entries of its local gradient onto a set of common orthogonal

View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 20/30

Undermind

REPORT CREATED ON
8/29/2025

waveforms and sends out the signal simultaneously to the edge server; owing to the limited number of orthogonal waveforms, only a subset of the parameters can
be selected for uploading during...

[94] Adaptive Convolutional ELM For Concept Drift Handling in Online Stream Data
Arif Budiman, ..., and Chan Basaruddin. ArXiv, 2016. 9 citations.
0% Topic Match
Abstract: In big data era, the data continuously generated and its distribution may keep changes overtime. These challenges in online stream of data are known
as concept drift. In this paper, we proposed the Adaptive Convolutional ELM method (ACNNELM) as enhancement of Convolutional Neural Network (CNN) with a
hybrid Extreme Learning Machine (ELM) model plus adaptive capability. This method is aimed for concept drift handling. We enhanced the CNN as convolutional
hiererchical features representation learner combined with Elastic ELM (E$^2$LM) as a parallel supervised classifier. We propose an Adaptive OS-ELM (AOS-ELM)
for concept drift adaptability in classifier level (named ACNNELM-1) and...

[95] Addressing Client Drift in Federated Continual Learning with Adaptive Optimization
Yeshwanth Venkatesha, ..., and P. Panda. ArXiv, 2022. 9 citations.
0% Topic Match
Abstract: Federated learning has been extensively studied and is the prevalent method for privacy-preserving distributed learning in edge devices. Correspondingly,
continual learning is an emerging field targeted towards learning multiple tasks sequentially. However, there is little attention towards additional challenges emerging
when federated aggregation is performed in a continual learning system. We identify \textit{client drift} as one of the key weaknesses that arise when vanilla federated
averaging is applied in such a system, especially since each client can independently have different order of tasks. We outline a framework for performing Federated
Continual Learning (FCL) by using NetTailor as a candidate continual...

[96] Efficient Lifelong Learning with A-GEM
Arslan Chaudhry, ..., and Mohamed Elhoseiny. ArXiv, 2018. 1513 citations.
0% Topic Match
Abstract: In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up
learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost.
Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection
is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric...

[97] Incremental learning with the minimum description length principle
Pierre-Alexandre Murena, ..., and J. Dessalles. 2017 International Joint Conference on Neural Networks (IJCNN), 2017. 1 citations.
0% Topic Match
Abstract: Whereas a large number of machine learning methods focus on offline learning over a single batch of data called training data set, the increasing number
of automatically generated data leads to the emergence of new issues that offline learning cannot cope with. Incremental learning designates online learning of a
model from streaming data. In non-stationary environments, the process generating these data may change over time, hence the learned concept becomes invalid.
Adaptation to this non-stationary nature, called concept drift, is an intensively studied topic and can be reached algorithmically by two opposite approaches: active
or passive approaches. We propose a...

[98] Progress & Compress: A scalable framework for continual learning
Jonathan Schwarz, ..., and R. Hadsell. ArXiv, 2018. 906 citations.
0% Topic Match
Abstract: We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in
the number of parameters and is designed to preserve performance on previously encountered tasks while accelerating learning progress on subsequent problems.
This is achieved by training a network with two components: A knowledge base, capable of solving previously encountered problems, which is connected to an
active column that is employed to efficiently learn the current task. After learning a new task, the active column is distilled into the knowledge base, taking care to
protect any previously acquired skills....

[99] Active Learning With Drifting Streaming Data
Indr Žliobait, ..., and G. Holmes. IEEE Transactions on Neural Networks and Learning Systems, 2014. 344 citations.
0% Topic Match
No summary or abstract available

[100] Age-Aware Partial Gradient Update Strategy for Federated Learning Over the Air
Ruihao Du, ..., and Howard H. Yang. ArXiv, 2025. 0 citations.
0% Topic Match
Abstract: We propose an age-aware strategy to update gradients in an over-the-air federated learning system. The system comprises an edge server and multiple
clients, collaborating to minimize a global loss function. In each communication round, clients perform local training, modulate their gradient updates onto a set of
shared orthogonal waveforms, and simultaneously transmit the analog signals to the edge server. The edge server then extracts a noisy aggregated gradient from
the received radio signal, updates the global model, and broadcasts it to the clients for the next round of local computing. Despite enabling all clients to upload
information in every communication...

[101] Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation
Chun-Peng Chang, ..., and Alain Pagani. ArXiv, 2025. 0 citations.
0% Topic Match
Abstract: Recent advancements in video generation have substantially improved visual quality and temporal coherence, making these models increasingly appealing
for applications such as autonomous driving, particularly in the context of driving simulation and so-called"world models". In this work, we investigate the effects of
existing fine-tuning video generation approaches on structured driving datasets and uncover a potential trade-off: although visual fidelity improves, spatial accuracy
in modeling dynamic elements may degrade. We attribute this degradation to a shift in the alignment between visual quality and dynamic understanding objectives.
In datasets with diverse scene structures within temporal space, where objects or perspective shift in...

[102] Continual Learning with Deep Generative Replay
Hanul Shin, ..., and Jiwon Kim. Unknown journal, 2017. 2152 citations.
0% Topic Match
Abstract: Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic
forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications
where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep
Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model ("generator") and a task solving model
("solver"). With only these two models,...

[103] Concept-drifting Data Streams are Time Series; The Case for Continuous Adaptation
J. Read. ArXiv, 2018. 16 citations.
0% Topic Match
Abstract: Learning from data streams is an increasingly important topic in data mining, machine learning, and artificial intelligence in general. A major focus in the data
stream literature is on designing methods that can deal with concept drift, a challenge where the generating distribution changes over time. A general assumption
in most of this literature is that instances are independently distributed in the stream. In this work we show that, in the context of concept drift, this assumption is
contradictory, and that the presence of concept drift necessarily implies temporal dependence; and thus some form of time series. This has important...

[104] Overcoming catastrophic forgetting in neural networks
J. Kirkpatrick, ..., and R. Hadsell. Proceedings of the National Academy of Sciences, 2016. 7945 citations.
0% Topic Match

View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 21/30

Undermind

REPORT CREATED ON
8/29/2025

Abstract: Significance Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language
translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially.
In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by
synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially. The ability to learn
tasks in a sequential fashion is crucial to the development of artificial...

[105] Learning to Learn with Feedback and Local Plasticity
Jack W Lindsey and Ashok Litwin-Kumar. ArXiv, 2020. 33 citations.
0% Topic Match
Abstract: Interest in biologically inspired alternatives to backpropagation is driven by the desire to both advance connections between deep learning and neuroscience
and address backpropagation's shortcomings on tasks such as online, continual learning. However, local synaptic learning rules like those employed by the brain have
so far failed to match the performance of backpropagation in deep networks. In this study, we employ meta-learning to discover networks that learn using feedback
connections and local, biologically inspired learning rules. Importantly, the feedback connections are not tied to the feedforward weights, avoiding biologically
implausible weight transport. Our experiments show that meta-trained networks effectively use...

[106] SWE-Bench-CL: Continual Learning for Coding Agents
Thomas Joshi, ..., and Fatih Uysal. ArXiv, 2025. 0 citations.
0% Topic Match
Abstract: Large Language Models (LLMs) have achieved impressive results on static code-generation benchmarks, but real-world software development unfolds as
a continuous stream of evolving issues, fixes, and feature requests. We introduce SWE-Bench-CL, a novel continual learning benchmark built on the human-verified
SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By organizing GitHub issues into chronologically ordered sequences that reflect
natural repository evolution, SWE-Bench-CL enables direct evaluation of an agent's ability to accumulate experience, transfer knowledge across tasks, and resist
catastrophic forgetting. We complement the dataset with (i) a preliminary analysis of inter-task structural similarity and contextual sensitivity, (ii) an...

[107] Streaming Batch Eigenupdates for Hardware Neural Networks
B. Hoskins, ..., and M. Stiles. Frontiers in Neuroscience, 2019. 15 citations.
0% Topic Match
Abstract: Neural networks based on nanodevices, such as metal oxide memristors, phase change memories, and flash memory cells, have generated considerable
interest for their increased energy efficiency and density in comparison to graphics processing units (GPUs) and central processing units (CPUs). Though immense
acceleration of the training process can be achieved by leveraging the fact that the time complexity of training does not scale with the network size, it is limited by
the space complexity of stochastic gradient descent, which grows quadratically. The main objective of this work is to reduce this space complexity by using low-rank
approximations of stochastic gradient...

[108] Challenges in Streaming Data Analysis for Building an Adaptive Model for Handling Concept Drifts
Shahad P Ebin and Deni Raj. 2021 International Conference on System, Computation, Automation and Networking (ICSCAN), 2021. 1 citations.
0% Topic Match
Abstract: Incremental learning from non-stationary environments with evolving streaming data is one of the main challenges in streaming data analysis (SDA). The
advances in data acquisition technology have ignited the importance of data analysis on streaming data. Analysis on streaming data has to be performed with
constrained memory and single pass over the data. The main challenges are in window adjustments, various drift detections, feature selection, dynamic model
selection, adaptability of models and it’s real-time processing. The change in characteristics of data stream often make our model obsolete. Such changes are
called concept drift. This concept drift mandates model adaptations in...

[109] FedASMU: Efficient Asynchronous Federated Learning with Dynamic Staleness-aware Model Update
Ji Liu, ..., and D. Dou. ArXiv, 2023. 46 citations.
0% Topic Match
Abstract: As a promising approach to deal with distributed data, Federated Learning (FL) achieves major advancements in recent years. FL enables collaborative
model training by exploiting the raw data dispersed in multiple edge devices. However, the data is generally non-independent and identically distributed, i.e.,
statistical heterogeneity, and the edge devices significantly differ in terms of both computation and communication capacity, i.e., system heterogeneity. The statistical
heterogeneity leads to severe accuracy degradation while the system heterogeneity significantly prolongs the training process. In order to address the heterogeneity
issue, we propose an Asynchronous Staleness-aware Model Update FL framework, i.e., FedASMU, with two novel...

[110] Learning under Concept Drift: A Review
Jie Lu, ..., and Guangquan Zhang. IEEE Transactions on Knowledge and Data Engineering, 2019. 1342 citations.
0% Topic Match
Abstract: Concept drift describes unforeseeable changes in the underlying distribution of streaming data over time. Concept drift research involves the development
of methodologies and techniques for drift detection, understanding, and adaptation. Data analysis has revealed that machine learning in a concept drift environment
will result in poor learning results if the drift is not addressed. To help researchers identify which research topics are significant and how to apply related techniques
in data analysis tasks, it is necessary that a high quality, instructive review of current research developments and trends in the concept drift field is conducted. In
addition, due to the...

[111] Lifelong Machine Learning with Deep Streaming Linear Discriminant Analysis
Tyler L. Hayes and Christopher Kanan. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2019.
145 citations.
0% Topic Match
Abstract: When an agent acquires new information, ideally it would immediately be capable of using that information to understand its environment. This is not
possible using conventional deep neural networks, which suffer from catastrophic forgetting when they are incrementally updated, with new knowledge overwriting
established representations. A variety of approaches have been developed that attempt to mitigate catastrophic forgetting in the incremental batch learning scenario,
where a model learns from a series of large collections of labeled samples. However, in this setting, inference is only possible after a batch has been accumulated,
which prohibits many applications. An alternative paradigm is online...

[112] Overcoming Forgetting in Federated Learning on Non-IID Data
N. Shoham, ..., and Itai Zeitak. ArXiv, 2019. 235 citations.
0% Topic Match
Abstract: We tackle the problem of Federated Learning in the non i.i.d. case, in which local models drift apart, inhibiting learning. Building on an analogy with Lifelong
Learning, we adapt a solution for catastrophic forgetting to Federated Learning. We add a penalty term to the loss function, compelling all local models to converge to
a shared optimum. We show that this can be done efficiently for communication (adding no further privacy risks), scaling with the number of nodes in the distributed
setting. Our experiments show that this method is superior to competing ones for image recognition on the MNIST dataset.

[113] Towards Robust Evaluations of Continual Learning
Sebastian Farquhar and Y. Gal. ArXiv, 2018. 309 citations.
0% Topic Match
Abstract: Experiments used in current continual learning research do not faithfully assess fundamental challenges of learning continually. Instead of assessing
performance on challenging and representative experiment designs, recent research has focused on increased dataset difficulty, while still using flawed experiment
set-ups. We examine standard evaluations and show why these evaluations make some continual learning approaches look better than they are. We introduce
desiderata for continual learning evaluations and explain why their absence creates misleading comparisons. Based on our desiderata we then propose new
experiment designs which we demonstrate with various continual learning approaches and datasets. Our analysis calls for a reprioritization...

[114] Continuous and Adaptive Learning over Big Streaming Data for Network Security
Pavol Mulinka, ..., and J. Vanerio. 2019 IEEE 8th International Conference on Cloud Networking (CloudNet), 2019. 4 citations.
0% Topic Match
Abstract: Continuous and adaptive learning is an effective learning approach when dealing with highly dynamic and changing scenarios, where concept drift often
happens. In a continuous, stream or adaptive learning setup, new measurements arrive continuously and there are no boundaries for learning, meaning that the

View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 22/30

Undermind

REPORT CREATED ON
8/29/2025

learning model has to decide how and when to (re)learn from these new data constantly. We address the problem of adaptive and continual learning for network
security, building dynamic models to detect network attacks in real network traffic. The combination of fast and big network measurements data with the re-training
paradigm of adaptive learning imposes complex...

[115] Advances and Open Problems in Federated Learning
P. Kairouz, ..., and Sen Zhao. Found. Trends Mach. Learn., 2019. 6616 citations.
0% Topic Match
Abstract: Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under
the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and
minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches.
Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.

[116] Federated Continual Learning through distillation in pervasive computing
Anastasiia Usmanova, ..., and Germán Vega. 2022 IEEE International Conference on Smart Computing (SMARTCOMP), 2022. 10 citations.
0% Topic Match
Abstract: Federated Learning has been introduced as a new machine learning paradigm enhancing the use of local devices. At a server level, FL regularly aggregates
models learned locally on distributed clients to obtain a more general model. Current solutions rely on the availability of large amounts of stored data at the client
side in order to fine-tune the models sent by the server. Such setting is not realistic in mobile pervasive computing where data storage must be kept low and data
characteristic can change dramatically. To account for this variability, a solution is to use the data regularly collected by the...

[117] Learning without Forgetting
Zhizhong Li and Derek Hoiem. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016. 4625 citations.
0% Topic Match
Abstract: When building a unified vision system or gradually adding new apabilities to a system, the usual assumption is that training data for all tasks is always
available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities
to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method,
which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature
extraction and...

[118] Generative replay with feedback connections as a general strategy for continual learning
Gido M. van de Ven and A. Tolias. ArXiv, 2018. 229 citations.
0% Topic Match
Abstract: A major obstacle to developing artificial intelligence applications capable of true lifelong learning is that artificial neural networks quickly or catastrophically
forget previously learned tasks when trained on a new one. Numerous methods for alleviating catastrophic forgetting are currently being proposed, but differences in
evaluation protocols make it difficult to directly compare their performance. To enable more meaningful comparisons, here we identified three distinct scenarios for
continual learning based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols
according to each of these scenarios,...

[119] Adaptive Explainable Continual Learning Framework for Regression Problems with Focus on Power Forecasts
Yujiang He. ArXiv, 2021. 2 citations.
0% Topic Match
Abstract: Compared with traditional deep learning techniques, continual learning enables deep neural networks to learn continually and adaptively. Deep neural
networks have to learn new tasks and overcome forgetting the knowledge obtained from the old tasks as the amount of data keeps increasing in applications. In this
article, two continual learning scenarios will be proposed to describe the potential challenges in this context. Besides, based on our previous work regarding the
CLeaR framework, which is short for continual learning for regression tasks, the work will be further developed to enable models to extend themselves and learn
data successively. Research topics are...

[120] Statistically Valid Post-Deployment Monitoring Should Be Standard for AI-Based Digital Health
Pavel Dolin, ..., and Visar Berisha. ArXiv, 2025. 2 citations.
0% Topic Match
Abstract: This position paper argues that post-deployment monitoring in clinical AI is underdeveloped and proposes statistically valid and label-efficient testing
frameworks as a principled foundation for ensuring reliability and safety in real-world deployment. A recent review found that only 9% of FDA-registered AI-based
healthcare tools include a post-deployment surveillance plan. Existing monitoring approaches are often manual, sporadic, and reactive, making them ill-suited for
the dynamic environments in which clinical models operate. We contend that post-deployment monitoring should be grounded in label-efficient and statistically valid
testing frameworks, offering a principled alternative to current practices. We use the term"statistically valid"to refer to methods...

[121] Task-Free Continual Learning
Rahaf Aljundi, ..., and T. Tuytelaars. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 370 citations.
0% Topic Match
Abstract: Methods proposed in the literature towards continual deep learning typically operate in a task-based sequential learning setup. A sequence of tasks is
learned, one at a time, with all data of current task available but not of previous or future tasks. Task boundaries and identities are known at all times. This setup,
however, is rarely encountered in practical applications. Therefore we investigate how to transform continual learning to an online setup. We develop a system that
keeps on learning over time in a streaming fashion, with data distributions gradually changing and without the notion of separate tasks. To this end,...

[122] Expert Gate: Lifelong Learning with a Network of Experts
Rahaf Aljundi, ..., and T. Tuytelaars. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 681 citations.
0% Topic Match
Abstract: In this paper we introduce a model of lifelong learning, based on a Network of Experts. New tasks / experts are learned and added to the model sequentially,
building on what was learned before. To ensure scalability of this process, data from previous tasks cannot be stored and hence is not available when learning a
new task. A critical issue in such context, not addressed in the literature so far, relates to the decision which expert to deploy at test time. We introduce a set of
gating autoencoders that learn a representation for the task at hand, and, at test...

[123] Discrete Key-Value Bottleneck
Frederik Trauble, ..., and B. Scholkopf. ArXiv, 2022. 23 citations.
0% Topic Match
Abstract: Deep neural networks perform well on classification tasks where data streams are i.i.d. and labeled data is abundant. Challenges emerge with non-stationary
training data streams such as continual learning. One powerful approach that has addressed this challenge involves pre-training of large encoders on volumes of
readily available data, followed by task-specific tuning. Given a new task, however, updating the weights of these encoders is challenging as a large number of
weights needs to be fine-tuned, and as a result, they forget information about the previous tasks. In the present work, we propose a model architecture to address
this issue, building...

[124] FedDAA: Dynamic Client Clustering for Concept Drift Adaptation in Federated Learning
Fu Peng and Ming Tang. ArXiv, 2025. 0 citations.
0% Topic Match
Abstract: In federated learning (FL), the data distribution of each client may change over time, introducing both temporal and spatial data heterogeneity, known as
concept drift. Data heterogeneity arises from three drift sources: real drift (a shift in the conditional distribution P(y|x)), virtual drift (a shift in the input distribution
P(x)), and label drift (a shift in the label distribution P(y)). However, most existing FL methods addressing concept drift primarily focus on real drift. When clients
experience virtual or label drift, these methods often fail to selectively retain useful historical knowledge, leading to catastrophic forgetting. A key challenge lies in
distinguishing...

[125] Gradient Episodic Memory for Continual Learning
David Lopez-Paz and Marc'Aurelio Ranzato. Unknown journal, 2017. 2838 citations.
View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 23/30

Undermind

REPORT CREATED ON
8/29/2025

0% Topic Match
Abstract: One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better
understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks.
First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also
in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM)...

[126] Optimizers Qualitatively Alter Solutions And We Should Leverage This
Razvan Pascanu, ..., and James Martens. ArXiv, 2025. 1 citations.
0% Topic Match
Abstract: Due to the nonlinear nature of Deep Neural Networks (DNNs), one can not guarantee convergence to a unique global minimum of the loss when using
optimizers relying only on local information, such as SGD. Indeed, this was a primary source of skepticism regarding the feasibility of DNNs in the early days of the
field. The past decades of progress in deep learning have revealed this skepticism to be misplaced, and a large body of empirical evidence shows that sufficiently
large DNNs following standard training protocols exhibit well-behaved optimization dynamics that converge to performant solutions. This success has biased the
community...

[127] A Continual Learning Survey: Defying Forgetting in Classification Tasks
Matthias De Lange, ..., and T. Tuytelaars. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019. 1894 citations.
0% Topic Match
Abstract: Artificial neural networks thrive in solving the classification problem for a particular rigid task, acquiring knowledge through generalized learning behaviour
from a distinct training phase. The resulting network resembles a static entity of knowledge, with endeavours to extend this knowledge without targeting the original
task resulting in a catastrophic forgetting. Continual learning shifts this paradigm towards networks that can continually accumulate knowledge over different tasks
without the need to retrain from scratch. We focus on task incremental classification, where tasks arrive sequentially and are delineated by clear boundaries. Our
main contributions concern: (1) a taxonomy and extensive overview of the...

[128] Deep Online Learning via Meta-Learning: Continual Adaptation for Model-Based RL
Anusha Nagabandi, ..., and S. Levine. ArXiv, 2018. 194 citations.
0% Topic Match
Abstract: Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can
adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this
capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural
network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization
algorithm with a Chinese...

[129] Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation
Manuel Röder, ..., and Frank-Michael Schleif. ArXiv, 2025. 0 citations.
0% Topic Match
Abstract: Federated Learning has emerged as a leading paradigm for decentralized, privacy-preserving learning, particularly relevant in the era of interconnected
edge devices equipped with sensors. However, the practical implementation of Federated Learning faces three primary challenges: the need for human involvement
in costly data labelling processes for target adaptation, covariate shift in client device data collection due to environmental factors affecting sensors, leading to
discrepancies between source and target samples, and the impracticality of continuous or regular model updates in resource-constrained environments due to
limited data transmission capabilities and technical constraints on channel availability and energy efficiency. To tackle these issues,...

[130] Forget but Recall: Incremental Latent Rectification in Continual Learning
Nghia D. Nguyen, ..., and Khoa D. Doan. ArXiv, 2024. 0 citations.
0% Topic Match
Abstract: Intrinsic capability to continuously learn a changing data stream is a desideratum of deep neural networks (DNNs). However, current DNNs suffer from
catastrophic forgetting, which hinders remembering past knowledge. To mitigate this issue, existing Continual Learning (CL) approaches either retain exemplars
for replay, regularize learning, or allocate dedicated capacity for new tasks. This paper investigates an unexplored CL direction for incremental learning called
Incremental Latent Rectification or ILR. In a nutshell, ILR learns to propagate with correction (or rectify) the representation from the current trained DNN backward
to the representation space of the old task, where performing predictive decisions is...

[131] New ensemble methods for evolving data streams
A. Bifet, ..., and Ricard Gavaldà. Unknown journal, 2009. 617 citations.
0% Topic Match
No summary or abstract available

[132] Continual Learning Through Synaptic Intelligence
Friedemann Zenke, ..., and S. Ganguli. Proceedings of machine learning research, 2017. 2768 citations.
0% Topic Match
No summary or abstract available

[133] Continual Lifelong Learning with Neural Networks: A Review
G. I. Parisi, ..., and Stefan Wermter. Neural networks : the official journal of the International Neural Network Society, 2018. 2990 citations.
0% Topic Match
No summary or abstract available

[134] AiGAS-dEVL-RC: An Adaptive Growing Neural Gas Model for Recurrently Drifting Unsupervised Data Streams
Maria Arostegi, ..., and J. Ser. ArXiv, 2025. 0 citations.
0% Topic Match
Abstract: Concept drift and extreme verification latency pose significant challenges in data stream learning, particularly when dealing with recurring concept changes
in dynamic environments. This work introduces a novel method based on the Growing Neural Gas (GNG) algorithm, designed to effectively handle abrupt recurrent
drifts while adapting to incrementally evolving data distributions (incremental drifts). Leveraging the self-organizing and topological adaptability of GNG, the proposed
approach maintains a compact yet informative memory structure, allowing it to efficiently store and retrieve knowledge of past or recurring concepts, even under
conditions of delayed or sparse stream supervision. Our experiments highlight the superiority of our...

[135] Early Drift Detection Method
Manuel Baena-Garc, ..., and R. Morales-Bueno. Unknown journal, 2005. 796 citations.
0% Topic Match
No summary or abstract available

[136] A survey on concept drift adaptation
J. Gama, ..., and A. Bouchachia. ACM Computing Surveys (CSUR), 2014. 2990 citations.
0% Topic Match
No summary or abstract available

[137] Rethinking Experience Replay: a Bag of Tricks for Continual Learning
Pietro Buzzega, ..., and S. Calderara. 2020 25th International Conference on Pattern Recognition (ICPR), 2020. 159 citations.
0% Topic Match
Abstract: In Continual Learning, a Neural Network is trained on a stream of data whose distribution shifts over time. Under these assumptions, it is especially
challenging to improve on classes appearing later in the stream while remaining accurate on previous ones. This is due to the infamous problem of catastrophic

View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 24/30

Undermind

REPORT CREATED ON
8/29/2025

forgetting, which causes a quick performance degradation when the classifier focuses on learning new categories. Recent literature proposed various approaches
to tackle this issue, often resorting to very sophisticated techniques. In this work, we show that naive rehearsal can be patched to achieve similar performance. We
point out some shortcomings that...

[138] Exploring Kolmogorov-Arnold Network Expansions in Vision Transformers for Mitigating Catastrophic Forgetting in Continual Learning
Zahid Ullah and Jihie Kim. ArXiv, 2025. 0 citations.
0% Topic Match
Abstract: Continual learning (CL), the ability of a model to learn new tasks without forgetting previously acquired knowledge, remains a critical challenge in artificial
intelligence, particularly for vision transformers (ViTs) utilizing Multilayer Perceptrons (MLPs) for global representation learning. Catastrophic forgetting, where new
information overwrites prior knowledge, is especially problematic in these models. This research proposes replacing MLPs in ViTs with Kolmogorov-Arnold Network
(KANs) to address this issue. KANs leverage local plasticity through spline-based activations, ensuring that only a subset of parameters is updated per sample,
thereby preserving previously learned knowledge. The study investigates the efficacy of KAN-based ViTs in CL scenarios...

[139] Continual Learning Beyond Experience Rehearsal and Full Model Surrogates
Prashant Bhat, ..., and Bahram Zonooz. ArXiv, 2025. 0 citations.
0% Topic Match
Abstract: Continual learning (CL) has remained a significant challenge for deep neural networks as learning new tasks erases previously acquired knowledge, either
partially or completely. Existing solutions often rely on experience rehearsal or full model surrogates to mitigate CF. While effective, these approaches introduce
substantial memory and computational overhead, limiting their scalability and applicability in real-world scenarios. To address this, we propose SPARC, a scalable CL
approach that eliminates the need for experience rehearsal and full-model surrogates. By effectively combining task-specific working memories and task-agnostic
semantic memory for cross-task knowledge consolidation, SPARC results in a remarkable parameter efficiency, using only 6%...

[140] Effective Learning in Dynamic Environments by Explicit Context Tracking
G. Widmer and M. Kubát. Unknown journal, 1993. 204 citations.
0% Topic Match
No summary or abstract available

[141] Continual learning for neural regression networks to cope with concept drift in industrial processes using convex optimisation
Wolfgang Grote-Ramm, ..., and Felix Schönig. Eng. Appl. Artif. Intell., 2023. 14 citations.
Not measured Topic Match
No summary or abstract available

[142] Concept drift detection and adaptation for federated and continual learning
F. Casado, ..., and S. Barro. Multimedia Tools and Applications, 2021. 74 citations.
Not measured Topic Match
Abstract: Smart devices, such as smartphones, wearables, robots, and others, can collect vast amounts of data from their environment. This data is suitable for
training machine learning models, which can significantly improve their behavior, and therefore, the user experience. Federated learning is a young and popular
framework that allows multiple distributed devices to train deep learning models collaboratively while preserving data privacy. Nevertheless, this approach may not
be optimal for scenarios where data distribution is non-identical among the participants or changes over time, causing what is known as concept drift. Little research
has yet been done in this field, but this...

[143] Evaluating and Explaining Generative Adversarial Networks for Continual Learning under Concept Drift
Filip Guzy, ..., and B. Krawczyk. 2021 International Conference on Data Mining Workshops (ICDMW), 2021. 6 citations.
Not measured Topic Match
Abstract: Generative Adversarial Networks (GANs) are among the most popular contemporary machine learning algorithms. Despite remarkable successes in their
developments, existing GANs cannot offer the appropriate tools to monitor their performance in a continual learning scenario when data distribution changes. We
propose a complete framework for monitoring and evaluating GANs during the continual learning, explaining their reaction to the data distribution shifts. The proposed
approach is the first complete solution for evaluating GANs in drifting environments, additionally adding explainability to the adaptation process. We introduce a
novel prequential metric for continual evaluation of GANs. We show how to use various information...

[144] Incremental Learning with Concept Drift Detection and Prototype-based Embeddings for Graph Stream Classification
Kleanthis Malialis, ..., and Marios M. Polycarpou. 2024 International Joint Conference on Neural Networks (IJCNN), 2024. 1 citations.
Not measured Topic Match
Abstract: Data stream mining aims at extracting meaningful knowledge from continually evolving data streams, addressing the challenges posed by nonstationary
environments, particularly, concept drift which refers to a change in the underlying data distribution over time. Graph structures offer a powerful modelling tool to
represent complex systems, such as, critical infrastructure systems and social networks. Learning from graph streams becomes a necessity to understand the
dynamics of graph structures and to facilitate informed decision-making. This work introduces a novel method for graph stream classification which operates under
the general setting where a data generating process produces graphs with varying nodes and...

[145] Concept Matching: Clustering-based Federated Continual Learning
Xiaopeng Jiang and C. Borcea. ArXiv, 2023. 0 citations.
Not measured Topic Match
Abstract: Federated Continual Learning (FCL) has emerged as a promising paradigm that combines Federated Learning (FL) and Continual Learning (CL). To
achieve good model accuracy, FCL needs to tackle catastrophic forgetting due to concept drift over time in CL, and to overcome the potential interference among
clients in FL. We propose Concept Matching (CM), a clustering-based framework for FCL to address these challenges. The CM framework groups the client models
into concept model clusters, and then builds different global models to capture different concepts in FL over time. In each round, the server sends the global concept
models to the clients....

[146] Machine learning in concept drift detection using statistical measures
Nail Adeeb Ali Abdu and K. O. Basulaim. International Journal of Computers and Applications, 2023. 2 citations.
Not measured Topic Match
Abstract: ABSTRACT In the data stream, the data has non-stationary quality because of continual and inconsistent change. This change is represented as the
concept drift in the classifying process of the streaming data. Representing this data drift concept in data stream mining requires pre-labeled samples. However,
labeling samples in real-time streaming (online) is not feasible due to resource utilization and time constraints. Therefore, this paper proposes the concept of
Probabilistic Concept Drift Detection (PCDD) in the group classifier. PCDD relies on the data stream classification process and provides concept drift without labeled
samples. The PCDD model is evaluated through an empirical...

[147] Adversarial vs behavioural-based defensive AI with joint, continual and active learning: automated evaluation of robustness to deception,
poisoning and concept drift
Alexandre Dey, ..., and Sylvain Navers. ArXiv, 2020. 1 citations.
Not measured Topic Match
Abstract: Recent advancements in Artificial Intelligence (AI) have brought new capabilities to behavioural analysis (UEBA) for cyber-security consisting in the
detection of hostile action based on the unusual nature of events observed on the Information this http URL our previous work (presented at C\&ESAR 2018 and FIC
2019), we have associated deep neural networks auto-encoders for anomaly detection and graph-based events correlation to address major limitations in UEBA
systems. This resulted in reduced false positive and false negative rates, improved alert explainability, while maintaining real-time performances and scalability.
However, we did not address the natural evolution of behaviours through time, also...

[148] Concept Drift Detection with Optimal Machine Learning Model for Data Classification
S. C. Emerald and T. Vengattaraman. 2022 6th International Conference on Trends in Electronics and Informatics (ICOEI), 2022. 1 citations.
Not measured Topic Match

View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 25/30

Undermind

REPORT CREATED ON
8/29/2025

Abstract: Recently, massive quantity of data streams have been produced rapidly due to the continual technological advancements and it poses concept drifting
(CD) as a challenging problem. Since the performance of the classification model gets degraded owing to the presence of concept drifting, it is needed to design
effective concept drift detection approaches. Classical classification models are not predicted to discover the pattern in a non-stationary data distribution. In real
time scenarios, the classification models are required to properly identify the concept drift and adapt over time. With this motivation, this article develops a new
concept of drift detection with chimp...

[149] ICICLE: Interpretable Class Incremental Continual Learning
Dawid Rymarczyk, ..., and BartBomiej Twardowski
. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 30 citations.
Not measured Topic Match
Abstract: Continual learning enables incremental learning of new tasks without forgetting those previously learned, resulting in positive knowledge transfer that can
enhance performance on both new and old tasks. However, continual learning poses new challenges for interpretability, as the rationale behind model predictions
may change over time, leading to interpretability concept drift. We address this problem by proposing Interpretable Class-InCremental LEarning (ICICLE), an
exemplar-free approach that adopts a prototypical part-based approach. It consists of three crucial novelties: interpretability regularization that distills previously
learned concepts while preserving user-friendly positive reasoning; proximity-based prototype initialization strategy dedicated to the fine-grained setting; and
task-recency bias...

[150] Continual Learning with Strategic Selection and Forgetting for Network Intrusion Detection
Xinchen Zhang, ..., and Shuang-Hua Yang. IEEE INFOCOM 2025 - IEEE Conference on Computer Communications, 2024. 2 citations.
Not measured Topic Match
Abstract: Intrusion Detection Systems (IDS) are crucial for safeguarding digital infrastructure. In dynamic network environments, both threat landscapes and normal
operational behaviors are constantly changing, resulting in concept drift. While continuous learning mitigates the adverse effects of concept drift, insufficient attention
to drift patterns and excessive preservation of outdated knowledge can still hinder the IDS's adaptability. In this paper, we propose SSF (Strategic Selection and
Forgetting), a novel continual learning method for IDS, providing continuous model updates with a constantly refreshed memory buffer. Our approach features a
strategic sample selection algorithm to select representative new samples and a strategic forgetting mechanism...

[151] Survey on Online Streaming Continual Learning
N. Gunasekara, ..., and A. Bifet. Unknown journal, 2023. 24 citations.
Not measured Topic Match
Abstract: Stream Learning (SL) attempts to learn from a data stream efficiently. A data stream learning algorithm should adapt to input data distribution shifts without
sacrificing accuracy. These distribution shifts are known as ”concept drifts” in the literature. SL provides many supervised, semi-supervised, and unsupervised
methods for detecting and adjusting to concept drift. On the other hand, Continual Learning (CL) attempts to preserve previous knowledge while performing well on
the current concept when confronted with concept drift. In Online Continual Learning (OCL), this learning happens online. This survey explores the intersection of
those two online learning paradigms to find synergies. We...

[152] Attentive Federated Learning for Concept Drift in Distributed 5G Edge Networks
A. Estiri and Muthucumaru Maheswaran. ArXiv, 2021. 9 citations.
Not measured Topic Match
Abstract: Machine learning (ML) is expected to play a major role in 5G edge computing. Various studies have demonstrated that ML is highly suitable for optimizing
edge computing systems as rapid mobility and application-induced changes occur at the edge. For ML to provide the best solutions, it is important to continually
train the ML models to include the changing scenarios. The sudden changes in data distributions caused by changing scenarios (e.g., 5G base station failures) is
referred to as concept drift and is a major challenge to continual learning. The ML models can present high error rates while the drifts take...

[153] Energy-efficient Online Continual Learning for Time Series Classification in Nanorobot-based Smart Health.
Le Sun, ..., and Prayag Tiwari. IEEE journal of biomedical and health informatics, 2023. 4 citations.
Not measured Topic Match
Abstract: Nanorobots have been used in smart health to collect time series data such as electrocardiograms and electroencephalograms. Real-time classification
of dynamic time series signals in nanorobots is a challenging task. Nanorobots in the nanoscale range require a classification algorithm with low computational
complexity. First, the classification algorithm should be able to dynamically analyze time series signals and update itself to process the concept drifts (CD). Second,
the classification algorithm should have the ability to handle catastrophic forgetting (CF) and classify historical data. Most importantly, the classification algorithm
should be energy-efficient to use less computing power and memory to classify signals...

[154] Real-World Applications of Continual Learning: From Theory to Practice
Tajinder Kumar, ..., and Sunita Rani. 2023 International Conference on Advanced Computing & Communication Technologies (ICACCTech),
2023. 0 citations.
Not measured Topic Match
Abstract: The area of continuous learning, which addresses the difficulty of learning and adapting in changing contexts, has attracted much interest lately. Although
continual learning approaches have seen significant theoretical breakthroughs, applying them in the real world is still difficult. This essay tries to close the knowledge
gap between theory and practice by thoroughly analyzing continual learning's practical implications. The authors give case studies and real-world examples from
various fields, such as computer vision, natural language processing, robotics, and online learning. We highlight the advantages, constraints, and lessons discovered
while implementing continuous learning in real-world situations by looking at these applications....

[155] TRICL: Triplet Continual Learning
Xianchao Zhang, ..., and Wentao Yang. ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2023. 0 citations.
Not measured Topic Match
Abstract: A class-incremental learning agent learns online with a neverending stream of data in one training epoch. In this setting, the agent suffers from severe
catastrophic forgetting due to the absence of data from the observed classes after learning data from new classes. Besides, the prototypes rapidly become outdated
as the agent adapts to new data sequentially, and the previous example embeddings spread out in an unforeseen way, which exacerbates forgetting (i.e., concept
drift). Based on this observation, we propose a replay-based method, called TriCL, which gathers the embeddings near the prototype from the same class and
separates the embeddings from...

[156] ROSE: robust online self-adjusting ensemble for continual learning on imbalanced drifting data streams
Alberto Cano and B. Krawczyk. Machine Learning, 2022. 86 citations.
Not measured Topic Match
No summary or abstract available

[157] Exploring the Intersection Between Neural Architecture Search and Continual Learning
M. Shahawy, ..., and David White. IEEE Transactions on Neural Networks and Learning Systems, 2022. 4 citations.
Not measured Topic Match
Abstract: Despite the significant advances achieved in deep learning, the deep neural networks’ (DNNs) design approach remains notoriously tedious, depending
primarily on intuition, experience, and trial and error. This human-dependent process is often time-consuming and prone to errors. Furthermore, the models are
generally bound to their training contexts, with no considerations to their surrounding environments. Continual adaptiveness and automation of neural networks is
of paramount importance to several domains where model accessibility is limited after deployment (e.g., IoT devices, self-driving vehicles, etc.). Additionally, even
accessible models require frequent maintenance postdeployment to overcome issues such as data/concept drift, which can be cumbersome...

[158] A multi-agent continual learning framework for skin cancer detection leveraging crowdsourced dermoscopic images
Mani Abedini. World Journal of Advanced Research and Reviews, 2025. 0 citations.
Not measured Topic Match
Abstract: Skin cancer represents one of the most common malignancies globally, making early detection crucial for effective treatment and improved patient
outcomes. While dermatologists typically rely on dermoscopy and clinical examinations for diagnosis, recent advances in artificial intelligence, specifically deep
learning techniques like convolutional neural networks (CNNs), have shown significant promise in automating skin lesion classification [1,2]. Although CNN models
trained on benchmark dermatological datasets such as HAM10000 and ISIC have demonstrated diagnostic accuracies comparable to expert dermatologists, their

View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 26/30

Undermind

REPORT CREATED ON
8/29/2025

effectiveness declines when faced with evolving real-world data distributions, a phenomenon known as concept drift [3,4]. To address the limitations associated
with...

[159] Tensor decision trees for continual learning from drifting data streams
B. Krawczyk. Machine Learning, 2021. 5 citations.
Not measured Topic Match
No summary or abstract available

[160] Enabling Continual Learning with Differentiable Hebbian Plasticity
Vithursan Thangarasa, ..., and Graham W. Taylor. 2020 International Joint Conference on Neural Networks (IJCNN), 2020. 15 citations.
Not measured Topic Match
Abstract: Continual learning is the problem of sequentially learning new tasks or knowledge while protecting previously acquired knowledge. However, catastrophic
forgetting poses a grand challenge for neural networks performing such learning process. Thus, neural networks that are deployed in the real world often
struggle in scenarios where the data distribution is non-stationary (concept drift), imbalanced, or not always fully available, i.e., rare edge cases. We propose
a Differentiable Hebbian Consolidation model which is composed of a Differentiable Hebbian Plasticity (DHP) Softmax layer that adds a rapid learning plastic
component (compressed episodic memory) to the fixed (slow changing) parameters of the softmax...

[161] An Incremental Construction of Deep Neuro Fuzzy System for Continual Learning of Nonstationary Data Streams
Mahardhika Pratama, ..., and Geoffrey I. Webb. IEEE Transactions on Fuzzy Systems, 2018. 52 citations.
Not measured Topic Match
Abstract: Existing fuzzy neural networks (FNNs) are mostly developed under a shallow network configuration having lower generalization power than those of
deep structures. This article proposes a novel self-organizing deep FNN, namely deep evolving fuzzy neural network (DEVFNN). Fuzzy rules can be automatically
extracted from data streams or removed if they play limited role during their lifespan. The structure of the network can be deepened on demand by stacking additional
layers using a drift detection method, which not only detects the covariate drift, variations of input space, but also accurately identifies the real drift, dynamic changes
of both feature space and...

[162] Adversarial Attacks for Drift Detection
Fabian Hinder, ..., and Barbara Hammer. ArXiv, 2024. 0 citations.
Not measured Topic Match
Abstract: Concept drift refers to the change of data distributions over time. While drift poses a challenge for learning models, requiring their continual adaption, it is
also relevant in system monitoring to detect malfunctions, system failures, and unexpected behavior. In the latter case, the robust and reliable detection of drifts is
imperative. This work studies the shortcomings of commonly used drift detection schemes. We show how to construct data streams that are drifting without being
detected. We refer to those as drift adversarials. In particular, we compute all possible adversairals for common detection schemes and underpin our theoretical
findings with empirical...

[163] Towards Efficient Learning on the Computing Continuum: Advancing Dynamic Adaptation of Federated Learning
Mathis Valli, ..., and Loïc Cudennec. Proceedings of the 14th Workshop on AI and Scientific Computing at Scale using Flexible Computing
Infrastructures, 2024. 0 citations.
Not measured Topic Match
Abstract: Federated Learning (FL) has emerged as a paradigm shift enabling heterogeneous clients and devices to collaborate on training a shared global model
while preserving the privacy of their local data. However, a common yet impractical assumption in existing FL approaches is that the deployment environment is
static, which is rarely true in heterogeneous and highly-volatile environments like the Edge-Cloud Continuum, where FL is typically executed. While most of the
current FL approaches process data in an online fashion, and are therefore adaptive by nature, they only support adaptation at the ML/DL level (e.g., through
continual learning to tackle data and...

[164] Continuously Learning Bug Locations
Paulina Stevia Nouwou Mindom, ..., and Foutse Khomh. ArXiv, 2024. 0 citations.
Not measured Topic Match
Abstract: Automatically locating buggy changesets associated with bug reports is crucial in the software development process. Deep Learning (DL)-based techniques
show promising results by leveraging structural information from the code and learning links between changesets and bug reports. However, since source code
associated with changesets evolves, the performance of such models tends to degrade over time due to concept drift. Aiming to address this challenge, in this
paper, we evaluate the potential of using Continual Learning (CL) techniques in multiple sub-tasks setting for bug localization (each of which operates on either
stationary or non-stationary data), comparing it against a bug localization...

[165] From Batch to Streaming: Building Real-time Inference Pipelines for Machine Learning
Chirag Maheshwari. International Journal of Scientific Research in Computer Science, Engineering and Information Technology, 2025. 0 citations.
Not measured Topic Match
Abstract: Modern machine learning applications are experiencing a fundamental shift from traditional batch processing toward real-time inference pipelines, driven
by the increasing demand for timely and context-aware predictions. This article comprehensively explores different training and serving architectures, ranging from
conventional batch processing to sophisticated streaming approaches. It examines the evolution of ML pipelines, discussing the advantages and challenges of
various architectural patterns, including batch training with batch predictions, batch training with streaming predictions, and fully streaming approaches. The article
delves into the implementation considerations for each architecture, addressing critical challenges such as data freshness, concept drift, and model degradation. It
also...

[166] Neuro-symbolic Meta Reinforcement Learning for Trading
S. Harini, ..., and L. Vig. ArXiv, 2023. 3 citations.
Not measured Topic Match
Abstract: We model short-duration (e.g. day) trading in financial markets as a sequential decision-making problem under uncertainty, with the added complication
of continual concept-drift. We, therefore, employ meta reinforcement learning via the RL2 algorithm. It is also known that human traders often rely on frequently
occurring symbolic patterns in price series. We employ logical program induction to discover symbolic patterns that occur frequently as well as recently, and explore
whether using such features improves the performance of our meta reinforcement learning algorithm. We report experiments on real data indicating that meta-RL
is better than vanilla RL and also benefits from learned...

[167] Minimax Forward and Backward Learning of Evolving Tasks with Performance Guarantees
Verónica Álvarez, ..., and J. A. Lozano. ArXiv, 2023. 2 citations.
Not measured Topic Match
Abstract: For a sequence of classification tasks that arrive over time, it is common that tasks are evolving in the sense that consecutive tasks often have a higher
similarity. The incremental learning of a growing sequence of tasks holds promise to enable accurate classification even with few samples per task by leveraging
information from all the tasks in the sequence (forward and backward learning). However, existing techniques developed for continual learning and concept drift
adaptation are either designed for tasks with time-independent similarities or only aim to learn the last task in the sequence. This paper presents incremental
minimax risk classifiers...

[168] Incremental Domain Learning for Surface Quality Inspection of Automotive High Voltage Battery
Majid Shirazi, ..., and Amr Risk. 2023 International Conference on Machine Learning and Applications (ICMLA), 2023. 0 citations.
Not measured Topic Match
Abstract: Catastrophic forgetting refers to a neural network's detrimental loss of previously learned information upon acquiring new knowledge. Recent continual
learning methodologies grant data scientists less severe effects of catastrophic forgetting by maximizing the preservation of knowledge on former training sets upon
new tasks. Current knowledge conservation solutions mainly aim to find an equilibrium cluster of inter-class model parameters while adding new classes. Instead,
this paper delivers remedy to an intra-class domain incremental learning setup against partial concept drift on high-voltage battery surfaces in the automotive
manufacturing. Due to forgetting caveats that concern the validity of naive fine-tuning, we empirically examine...

[169] Applications of Sequential Learning for Medical Image Classification
View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 27/30

Undermind

REPORT CREATED ON
8/29/2025

Sohaib Naim, ..., and Craig K. Jones. ArXiv, 2023. 0 citations.
Not measured Topic Match
Abstract: Purpose: The aim of this work is to develop a neural network training framework for continual training of small amounts of medical imaging data and create
heuristics to assess training in the absence of a hold-out validation or test set. Materials and Methods: We formulated a retrospective sequential learning approach
that would train and consistently update a model on mini-batches of medical images over time. We address problems that impede sequential learning such as
overfitting, catastrophic forgetting, and concept drift through PyTorch convolutional neural networks (CNN) and publicly available Medical MNIST and NIH Chest
X-Ray imaging datasets. We begin by...

[170] Supervised Learning from Data Streams: An Overview and Update
Jesse Read and Indr.e vZliobait.e. ACM Computing Surveys, 2022. 4 citations.
Not measured Topic Match
Abstract: The literature on machine learning in the context of data streams is vast and growing. This indicates not only an ongoing interest, but also an ongoing need
for a synthesis of new developments in this area. Here we reformulate the definitions of supervised data-stream learning, alongside consideration of contemporary
concept drift and temporal dependence. Equipped with this, carry out a fresh discussion of what constitutes a supervised data-stream learning task; including
continual and reinforcement learning; highlighting major assumptions and constraints. We carry out a fresh reconsideration of approaches and methods, with regard
to their suitability to modern settings. But more...

[171] TimeLMs: Diachronic Language Models from Twitter
Daniel Loureiro, ..., and José Camacho-Collados. Unknown journal, 2022. 275 citations.
Not measured Topic Match
Abstract: Despite its importance, the time variable has been largely neglected in the NLP and language model literature. In this paper, we present TimeLMs, a set
of language models specialized on diachronic Twitter data. We show that a continual learning strategy contributes to enhancing Twitter-based language models’
capacity to deal with future and out-of-distribution tweets, while making them competitive with standardized and more monolithic benchmarks. We also perform a
number of qualitative analyses showing how they cope with trends and peaks in activity involving specific named entities or concept drift. TimeLMs is available at
github.com/cardiffnlp/timelms.

[172] Unsupervised Assessment of Landscape Shifts Based on Persistent Entropy and Topological Preservation
Sebastián Basterrech. Unknown journal, 2024. 0 citations.
Not measured Topic Match
Abstract: In Continual Learning (CL) contexts, concept drift typically refers to the analysis of changes in data distribution. A drift in the input data can have negative
consequences on a learning predictor and the system's stability. The majority of concept drift methods emphasize the analysis of statistical changes in non-stationary
data over time. In this context, we consider another perspective, where the concept drift also integrates substantial changes in the topological characteristics of
the data stream. In this article, we introduce a novel framework for monitoring changes in multi-dimensional data streams. We explore variations in the topological
structures of the data,...

[173] Decision Boundary-aware Knowledge Consolidation Generates Better Instance-Incremental Learner
Qiang Nie, ..., and Chengjie Wang. ArXiv, 2024. 0 citations.
Not measured Topic Match
Abstract: Instance-incremental learning (IIL) focuses on learning continually with data of the same classes. Compared to class-incremental learning (CIL), the IIL
is seldom explored because IIL suffers less from catastrophic forgetting (CF). However, besides retaining knowledge, in real-world deployment scenarios where
the class space is always predefined, continual and cost-effective model promotion with the potential unavailability of previous data is a more essential demand.
Therefore, we first define a new and more practical IIL setting as promoting the model's performance besides resisting CF with only new observations. Two issues
have to be tackled in the new IIL setting: 1) the notorious...

[174] CLFace: A Scalable and Resource-Efficient Continual Learning Framework for Lifelong Face Recognition
M. Hasan, ..., and Nasser M. Nasrabadi. 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2024. 0 citations.
Not measured Topic Match
Abstract: An important aspect of deploying face recognition (FR) algorithms in real-world applications is their ability to learn new face identities from a continuous
data stream. However, the online training of existing deep neural network-based FR algorithms, which are pre-trained offline on large-scale stationary datasets,
encounter two major challenges: (I) catastrophic forgetting of previously learned identities, and (II) the need to store past data for complete retraining from scratch,
leading to significant storage constraints and privacy concerns. In this paper, we introduce CLFace, a continual learning framework designed to preserve and
incrementally extend the learned knowledge. CLFace eliminates the classification layer,...

[175] On-device edge learning for IoT data streams: a survey
Afonso Lourencco, ..., and G. Marreiros. ArXiv, 2025. 1 citations.
Not measured Topic Match
Abstract: This literature review explores continual learning methods for on-device training in the context of neural networks (NNs) and decision trees (DTs) for
classification tasks on smart environments. We highlight key constraints, such as data architecture (batch vs. stream) and network capacity (cloud vs. edge),
which impact TinyML algorithm design, due to the uncontrolled natural arrival of data streams. The survey details the challenges of deploying deep learners on
resource-constrained edge devices, including catastrophic forgetting, data inefficiency, and the difficulty of handling IoT tabular data in open-world settings. While
decision trees are more memory-efficient for on-device training, they are limited in...

[176] Quantum Kernel methods for anomaly detection in high-velocity data streams
Kamal Singh Bisht. World Journal of Advanced Engineering Technology and Sciences, 2025. 0 citations.
Not measured Topic Match
Abstract: Quantum Kernel Methods for Anomaly Detection in High-Velocity Data Streams introduces a novel framework leveraging quantum computing principles
to address critical challenges in real-time anomaly detection. By combining the expressive power of quantum-enhanced feature spaces with classical machine
learning techniques, the work presents a hybrid architecture capable of identifying subtle anomalies in complex, high-dimensional streaming data. The framework
incorporates specialized quantum feature maps that efficiently encode temporal and distributional properties of data streams, while adaptation mechanisms respond
to concept drift and evolving patterns. Through systematic experimental evaluation across synthetic and real-world datasets from financial transactions, network
security, and industrial systems,...

[177] AI 2007 : Advances in artificial intelligence : 20th Australian Joint Conference on Artificial Intelligence Gold Coast, Australia, December 2-6,
2007 : proceedings
M. Orgun and J. Thornton. Unknown journal, 2007. 88 citations.
Not measured Topic Match
No summary or abstract available

[178] A Self-organizing Multi-agent System for Adaptive Continuous Unsupervised Learning in Complex Uncertain Environments
Igor Kiselev and R. Alhajj. Unknown journal, 2008. 13 citations.
Not measured Topic Match
No summary or abstract available

[179] Web technologies research and development - APWeb 2005 : 7th Asia-Pacific Web Conference, Shanghai, China, March 29-April 1, 2005 :
proceedings
Yanchun Zhang, ..., and Minglu Li. Unknown journal, 2005. 0 citations.
Not measured Topic Match
No summary or abstract available

[181] Contrastive Regularization over LoRA for Multimodal Biomedical Image Incremental Learning
Haojie Zhang, ..., and Shichao Kan. ArXiv, 2025. 0 citations.
View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 28/30

Undermind

REPORT CREATED ON
8/29/2025

Not measured Topic Match
Abstract: Multimodal Biomedical Image Incremental Learning (MBIIL) is essential for handling diverse tasks and modalities in the biomedical domain, as training
separate models for each modality or task significantly increases inference costs. Existing incremental learning methods focus on task expansion within a single
modality, whereas MBIIL seeks to train a unified model incrementally across modalities. The MBIIL faces two challenges: I) How to preserve previously learned
knowledge during incremental updates? II) How to effectively leverage knowledge acquired from existing modalities to support new modalities? To address these
challenges, we propose MSLoRA-CR, a method that fine-tunes Modality-Specific LoRA modules while incorporating Contrastive...

[184] A Survey of Continual Reinforcement Learning
Chaofan Pan, ..., and Jiye Liang. ArXiv, 2025. 1 citations.
Not measured Topic Match
Abstract: Reinforcement Learning (RL) is an important machine learning paradigm for solving sequential decision-making problems. Recent years have witnessed
remarkable progress in this field due to the rapid development of deep neural networks. However, the success of RL currently relies on extensive training data
and computational resources. In addition, RL's limited ability to generalize across tasks restricts its applicability in dynamic and real-world environments. With the
arisen of Continual Learning (CL), Continual Reinforcement Learning (CRL) has emerged as a promising research direction to address these limitations by enabling
agents to learn continuously, adapt to new tasks, and retain previously acquired knowledge....

[188] GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay
Yunan Zhang, ..., and Qingcai Chen. ArXiv, 2025. 0 citations.
Not measured Topic Match
Abstract: The continual learning capability of large language models (LLMs) is crucial for advancing artificial general intelligence. However, continual fine-tuning LLMs
across various domains often suffers from catastrophic forgetting, characterized by: 1) significant forgetting of their general capabilities, and 2) sharp performance
declines in previously learned tasks. To simultaneously address both issues in a simple yet stable manner, we propose General Sample Replay (GeRe), a framework
that use usual pretraining texts for efficient anti-forgetting. Beyond revisiting the most prevalent replay-based practices under GeRe, we further leverage neural states
to introduce a enhanced activation states constrained optimization method using threshold-based margin (TM)...

[189] Online Continual Graph Learning
Giovanni Donghi, ..., and Nicoló Navarin. ArXiv, 2025. 0 citations.
Not measured Topic Match
Abstract: The aim of Continual Learning (CL) is to learn new tasks incrementally while avoiding catastrophic forgetting. Online Continual Learning (OCL) specifically
focuses on learning efficiently from a continuous stream of data with shifting distribution. While recent studies explore Continual Learning on graphs exploiting Graph
Neural Networks (GNNs), only few of them focus on a streaming setting. Yet, many real-world graphs evolve over time, often requiring timely and online predictions.
Current approaches, however, are not well aligned with the standard OCL setting, partly due to the lack of a clear definition of online Continual Learning on graphs.
In this work, we...

[191] Rethinking Prompt Optimization: Reinforcement, Diversification, and Migration in Blackbox LLMs
Mohammad-Javad Davari, ..., and Eugene Belilovsky. ArXiv, 2025. 1 citations.
Not measured Topic Match
Abstract: An increasing number of NLP applications interact with large language models (LLMs) through black-box APIs, making prompt engineering critical for
controlling model outputs. While recent Automatic Prompt Optimization (APO) methods iteratively refine prompts using model-generated feedback, textual gradients,
they primarily focus on error correction and neglect valuable insights from correct predictions. This limits both their effectiveness and efficiency. In this paper, we
propose a novel APO framework centered on enhancing the feedback mechanism. We reinterpret the textual gradient as a form of negative reinforcement and
introduce the complementary positive reinforcement to explicitly preserve beneficial prompt components identified through successful predictions....

[192] ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining
Seonwu Kim, ..., and Byoung-Ki Jeon. ArXiv, 2025. 0 citations.
Not measured Topic Match
Abstract: The emergence of open-source large language models (LLMs) has expanded opportunities for enterprise applications; however, many organizations still
lack the infrastructure to deploy and maintain large-scale models. As a result, small LLMs (sLLMs) have become a practical alternative, despite their inherent
performance limitations. While Domain Adaptive Continual Pretraining (DACP) has been previously explored as a method for domain adaptation, its utility in
commercial applications remains under-examined. In this study, we validate the effectiveness of applying a DACP-based recipe across diverse foundation models
and service domains. Through extensive experiments and real-world evaluations, we demonstrate that DACP-applied sLLMs achieve substantial gains in...

[193] Towards Unsupervised Sudden Data Drift Detection in Federated Learning with Fuzzy Clustering
Morris Stallmann, ..., and Gerhard Weiss. 2024 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2024. 2 citations.
Not measured Topic Match
Abstract: Federated learning (FL) is a machine learning (ML) discipline that allows to train ML models on distributed data without revealing raw data instances.
It promises to enable ML in environments with data sharing constraints, e.g., due to data privacy concerns, or other considerations. Data and concept drift are
commonly referred to as unpredictable changes in data distributions over time. It is known to impact a ML model's performances in many real-world scenarios. While
drift detection and adaptation has been studied extensively in the non-federated setting, it is still less explored in the FL setting. The private and distributed nature
of...

[195] Investigation of D-Wave quantum annealing for training Restricted Boltzmann Machines and mitigating catastrophic forgetting
Abdelmoula El-Yazizi and Y. Koshka. ArXiv, 2025. 0 citations.
Not measured Topic Match
Abstract: Modest statistical differences between the sampling performances of the D-Wave quantum annealer (QA) and the classical Markov Chain Monte Carlo
(MCMC), when applied to Restricted Boltzmann Machines (RBMs), are explored to explain, and possibly address, the absence of significant and consistent
improvements in RBM trainability when the D-Wave sampling was used in previous investigations. A novel hybrid sampling approach, combining the classical and
the QA contributions, is investigated as a promising way to benefit from the modest differences between the two sampling methods. No improvements in the RBM
training are achieved in this work, thereby suggesting that the differences between...

[197] Information-Theoretic Generalization Bounds of Replay-based Continual Learning
Wen Wen, ..., and Yong-Jin Liu. ArXiv, 2025. 0 citations.
Not measured Topic Match
Abstract: Continual learning (CL) has emerged as a dominant paradigm for acquiring knowledge from sequential tasks while avoiding catastrophic forgetting. Although
many CL methods have been proposed to show impressive empirical performance, the theoretical understanding of their generalization behavior remains limited,
particularly for replay-based approaches. In this paper, we establish a unified theoretical framework for replay-based CL, deriving a series of information-theoretic
bounds that explicitly characterize how the memory buffer interacts with the current task to affect generalization. Specifically, our hypothesis-based bounds reveal
that utilizing the limited exemplars of previous tasks alongside the current task data, rather than exhaustive replay, facilitates...

[199] Overcoming Forgetting Using Adaptive Federated Learning for IIoT Devices With Non-IID Data
Benteng Zhang, ..., and Jie Wu. IEEE Internet of Things Journal, 2025. 0 citations.
Not measured Topic Match
Abstract: In real-world Industrial Internet of Things (IIoT) scenarios, due to the limited storage capacity of IIoT devices, fresh data continuously received by diverse
devices will overwrite the outdated data and change the local data distribution. However, state-of-the-art studies have demonstrated that federated learning tends
to focus on training with fresh data, and the latest global model may forget the historical update directions (i.e., catastrophic forgetting). This issue can significantly
degrade the global model accuracy. Existing methods primarily focus on integrating outdated data characteristics into fresh data but overlook the large parameter
update gap between global and local models during global...

[200] Federated Class-Incremental Learning via Weighted Aggregation and Distillation
Feng Wu, ..., and Yuanlu Chen. IEEE Internet of Things Journal, 2025. 1 citations.
Not measured Topic Match
View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 29/30

Undermind

REPORT CREATED ON
8/29/2025

Abstract: Federated Class-Incremental Learning (FCIL) aims to design privacy-preserving collaborative training methods to continuously learn new classes from
distributed datasets. In these scenarios, federated clients face the challenge of encountering new classes while being constrained by limited memory capacity, which
can lead to catastrophic forgetting in the resulting global model. Existing FCIL approaches tend to overlook the challenges posed by the heterogeneity of dataset
label distribution among clients, thereby constraining the generalization capacity of the global model they learn. Some of these methods also suffer from excessive
computational burdens when addressing catastrophic forgetting problems. Furthermore, certain approaches are constrained to handling...

[205] Unlocking the Power of Rehearsal in Continual Learning: A Theoretical Perspective
Junze Deng, ..., and Ness B. Shroff. ArXiv, 2025. 0 citations.
Not measured Topic Match
Abstract: Rehearsal-based methods have shown superior performance in addressing catastrophic forgetting in continual learning (CL) by storing and training on
a subset of past data alongside new data in current task. While such a concurrent rehearsal strategy is widely used, it remains unclear if this approach is always
optimal. Inspired by human learning, where sequentially revisiting tasks helps mitigate forgetting, we explore whether sequential rehearsal can offer greater benefits
for CL compared to standard concurrent rehearsal. To address this question, we conduct a theoretical analysis of rehearsal-based CL in overparameterized linear
models, comparing two strategies: 1) Concurrent Rehearsal, where past and...

View this report online at:
https://app.undermind.ai/report/362ea330a5c74629035c313b2e890e17952faab011cc36886d4e1ac2def03624

Page 30/30

